{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a first attempt at a rudimentary sentence grammaticality classifier \n",
    "# It is meant as a feasibility test for the larger experiment and as a learning\n",
    "# Experience for future endeavours\n",
    "\n",
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk import word_tokenize\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    #tokenizer = MosesTokenizer()\n",
    "    #tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in real_text]\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in real_text]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<5:\n",
    "        length = 5\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        self.hstate = self.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "def train_model(train_data,\n",
    "                embed_dim,\n",
    "                lstm_dim,\n",
    "                hidden_dim,\n",
    "                word_to_ix,\n",
    "                epochs,\n",
    "                learning_rate):\n",
    "    voc_size = len(word_to_ix)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim).cuda() \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word_to_ix,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"\\t Epoch{}:{}\".format(i,epoch_loss))\n",
    "    return linguo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing, testing\n",
    "def test_model(test_data,model,word2id):\n",
    "    correct = 0.0\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for testcase in test_data:\n",
    "        target = testcase[1]\n",
    "        prepared_inputs = prepare_input(word2id, testcase[0] )\n",
    "        prediction_vec = model(prepared_inputs).view(2)\n",
    "        if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if prediction == testcase[1]:\n",
    "            correct += 1\n",
    "            if target == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                fn +=1\n",
    "            else:\n",
    "                fp +=1\n",
    "                \n",
    "    # Compile results\n",
    "    accuracy = correct/len(test_data)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    fmeasure = 2*tp / (2*tp+fp+fn) \n",
    "    results = {\"accuracy\":accuracy,\n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "               \"fmeasure\":fmeasure,\n",
    "              \"tp\":tp,\n",
    "              \"tn\":tn,\n",
    "              \"fp\":fp,\n",
    "              \"fn\":fn}\n",
    "    return results\n",
    "\n",
    "def average_results(result_list):\n",
    "    total = len(result_list)\n",
    "    averaged =defaultdict(float)\n",
    "    for report in result_list:\n",
    "        for item in report:\n",
    "            averaged[item] += report[item]\n",
    "    for item in averaged:\n",
    "        averaged[item] = averaged[item]/total\n",
    "    return average_results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 947 sentences, 53 were dumped, among which 33 interogatives or exclamatives\n",
      "Your corpus has 947 grammatical sentences\n",
      "Word salad data has been generated for order 2\n",
      "\t947 word salads generated in 1.046 seconds\n",
      "Starting training on fold 1 for 2-grams...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c3e5e80f7431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                             \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                             learning_rate)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training finished in {:.4f} seconds, starting testing...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-828be4ffddb7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, embed_dim, lstm_dim, hidden_dim, word_to_ix, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvoc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlinguo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinguo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinguo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         raise RuntimeError(\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pablo/utilities/anaconda2/envs/python36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# This cell runs the full experiment with cross validation\n",
    "\n",
    "# Modify parameters here\n",
    "corpus_name = \"euro.mini\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 5\n",
    "train_proportion = 0.8\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "hidden_dim = 32\n",
    "epochs = 3\n",
    "learning_rate=0.1\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in prepro_gram]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "full_results = []\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(2,max_ngram+1):\n",
    "    t2 = time()\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in range(nsal)]\n",
    "    \n",
    "    labeled_g = [[sentence,1] for sentence in prepro_gram]\n",
    "    labeled_ws = [[sentence,0] for sentence in word_salads]\n",
    "    \n",
    "    message = \"Word salad data has been generated for order {}\".format(n)\n",
    "    print(message)\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,\n",
    "                                                                      te)\n",
    "    print(message)\n",
    "    message= \"Starting experiment on {}-grams ...\".format(n)\n",
    "    \n",
    "    result_list = []\n",
    "    # Iterate over the number of folds\n",
    "    for fold in range(folds):\n",
    "        t1 = time()\n",
    "        message = \"Starting training on fold {} for {}-grams...\".format(fold+1,n)\n",
    "        print(message)\n",
    "        # Shuffle and split data\n",
    "        random.shuffle(labeled_g)\n",
    "        random.shuffle(labeled_ws)\n",
    "        cutoff = math.floor(train_proportion * len(labeled_g))\n",
    "        train_g, test_g = labeled_g[:cutoff],labeled_g[cutoff:]\n",
    "        train_ws,test_ws = labeled_ws[:cutoff],labeled_ws[cutoff:]\n",
    "        \n",
    "        train_data = train_g + train_ws\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        test_data = test_g + test_ws\n",
    "        random.shuffle(test_data)\n",
    "        \n",
    "        # Train the Model\n",
    "        model = train_model(train_data,\n",
    "                            embed_dim,\n",
    "                            lstm_dim,\n",
    "                            hidden_dim,\n",
    "                            word2id,\n",
    "                            epochs,\n",
    "                            learning_rate)\n",
    "        te = time()-t1\n",
    "        message = \"Training finished in {:.4f} seconds, starting testing...\".format(te)\n",
    "        print(message)\n",
    "        t1 = time()\n",
    "        # Test the Model\n",
    "        fold_results = test_model(test_data,model,word2id)\n",
    "        result_list.append(fold_results)\n",
    "        te = time()-t1\n",
    "        message = \"Testing finished in {} seconds\".format(te)\n",
    "        \n",
    "        \n",
    "    order_results = average_results(result_list)\n",
    "    te2 = time()- t2\n",
    "    message=\"Results are in for {}-grams\".format(n)\n",
    "    print(message)\n",
    "    message=\"\\tFinished {} folds in {:.4f} s\".format(folds,te2)\n",
    "    print(message)\n",
    "    message=\"\\tAverage accuracy is:{}\".format(order_results[\"accuracy\"])\n",
    "    print(message)\n",
    "    message=\"\\tAverage F measure is:{}\".format(order_results[\"fmeasure\"])\n",
    "    print(message)\n",
    "    full_results.append(order_results)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: euro.mini\n",
    "Ngram-Order: 4 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:504.7399406191398\n",
    "Accuracy: 0.7973684210526316\n",
    "\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 10\n",
    "Final loss:232.77821091443258\n",
    "Accuracy: 0.6868421052631579\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:522.7681364201596\n",
    "Accuracy: 0.7236842105263158\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 2 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:363.4900658682709\n",
    "Accuracy: 0.8184210526315789\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 6 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:361.24798116292686\n",
    "Accuracy: 0.8552631578947368\n",
    "\n",
    "\n",
    "\n",
    "Type           | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy | \n",
    ":--------------|:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 51     | 0.97    |\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |\n",
    "Gram Vs Unigram|euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |\n",
    "Gram Vs Bigram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 718    | 0.81    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 840.8  | 0.67    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 673    | 0.75    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 490.9  | 0.75    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "time.sleep(70)\n",
    "t1 = time.time() - t0\n",
    "end = time.struct_time(t1)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
