{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the capacity of LSTM with word embeddings to detect syntactic structure via binary decision tasks\n",
    "## Presented by Pablo Gonzaalez Martinez\n",
    "\n",
    "This notebook contains the core code used for the experiments presented in the final project: \"Was that a sentence? Exploring the capacity of LSTM with word embeddings to detect syntactic structure via binary decision tasks\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "from nltk import word_tokenize\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing tokenizes and randomizes the order of the sentences, it then extracts low frequency items and replaces those tokens with the unknown token. Data generation produces the n-gram noise sentences to be used in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    #tokenizer = MosesTokenizer()\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in real_text]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    while length<7:\n",
    "        length= math.floor(random.gauss(avg_length, sd))\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    while length<7:\n",
    "        length= math.floor(random.gauss(avg_length, sd))\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        self.hstate = self.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "def train_model(train_data,\n",
    "                embed_dim,\n",
    "                lstm_dim,\n",
    "                hidden_dim,\n",
    "                word_to_ix,\n",
    "                epochs,\n",
    "                learning_rate):\n",
    "    voc_size = len(word_to_ix)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word_to_ix,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"\\t Epoch{}:{}\".format(i,epoch_loss))\n",
    "    return linguo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing, testing\n",
    "def test_model(test_data,model,word2id):\n",
    "    correct = 0.0\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for testcase in test_data:\n",
    "        target = testcase[1]\n",
    "        prepared_inputs = prepare_input(word2id, testcase[0] )\n",
    "        prediction_vec = model(prepared_inputs).view(2)\n",
    "        if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if prediction == testcase[1]:\n",
    "            correct += 1\n",
    "            if target == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                fn +=1\n",
    "            else:\n",
    "                fp +=1\n",
    "                \n",
    "    # Compile results\n",
    "    accuracy = correct/len(test_data)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    fmeasure = 2*tp / (2*tp+fp+fn) \n",
    "    results = {\"accuracy\":accuracy,\n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "               \"fmeasure\":fmeasure,\n",
    "              \"tp\":tp,\n",
    "              \"tn\":tn,\n",
    "              \"fp\":fp,\n",
    "              \"fn\":fn}\n",
    "    return results\n",
    "\n",
    "def average_results(result_list):\n",
    "    total = len(result_list)\n",
    "    averaged =defaultdict(float)\n",
    "    for report in result_list:\n",
    "        for item in report:\n",
    "            averaged[item] += report[item]\n",
    "    for item in averaged:\n",
    "        averaged[item] = averaged[item]/total\n",
    "    return averaged\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 947 sentences, 53 were dumped, among which 33 interogatives or exclamatives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 68/947 [00:00<00:01, 674.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your corpus has 947 grammatical sentences\n",
      "Grammatical corpus loaded in 1.587 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 947/947 [00:01<00:00, 694.83it/s]\n",
      "  1%|          | 10/947 [00:00<00:09, 98.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t947 word salads generated in 1.389 seconds\n",
      "Word salad data has been generated for order 2 and saved in Data/euro.mini_2-gramsWS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 947/947 [00:05<00:00, 160.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t947 word salads generated in 5.938 seconds\n",
      "Word salad data has been generated for order 3 and saved in Data/euro.mini_3-gramsWS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Modify parameters here\n",
    "#corpus_name = \"euro.hundo2\"\n",
    "corpus_name = \"euro.mini\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 10\n",
    "train_proportion = 0.8\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "hidden_dim = 32\n",
    "epochs = 3\n",
    "learning_rate=0.1\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "print(message)\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in prepro_gram]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "full_results = []\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(2,4):\n",
    "    t2 = time()\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in tqdm(range(nsal))]\n",
    "    \n",
    "    labeled_g = [[sentence,1] for sentence in prepro_gram]\n",
    "    labeled_ws = [[sentence,0] for sentence in word_salads]\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,te)\n",
    "    print(message)\n",
    "    outputWS_fname = \"Data/{}_{}-gramsWS\".format(corpus_name, n) \n",
    "   \n",
    "    with open(outputWS_fname,\"w\") as wsfile:\n",
    "        for labeled_sent in labeled_ws:\n",
    "            tokens = labeled_sent[0]\n",
    "            while tokens[0] == \"#\":\n",
    "                tokens.pop(0)\n",
    "            sentence = \" \".join(tokens) + \"\\n\"\n",
    "            wsfile.write(sentence)\n",
    "\n",
    "    message = \"Word salad data has been generated for order {} and saved in {} \".format(n,outputWS_fname)\n",
    "    print(message)\n",
    "   \n",
    " \n",
    "    \n",
    "\n",
    "output_fname = \"Data/\"+corpus_name + \"-pretrain\"\n",
    "with open(output_fname,\"w\") as wsfile:\n",
    "    for labeled_sent in labeled_g:\n",
    "        tokens = labeled_sent[0]\n",
    "        while tokens[0] == \"#\":\n",
    "            tokens.pop(0)\n",
    "        sentence = \" \".join(tokens) + \"\\n\"\n",
    "        wsfile.write(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euro.mini-pretrain'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fname\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_g)\n",
    "random.shuffle(labeled_ws)\n",
    "cutoff = math.floor(train_proportion * len(labeled_g))\n",
    "train_g, test_g = labeled_g[:cutoff],labeled_g[cutoff:]\n",
    "train_ws,test_ws = labeled_ws[:cutoff],labeled_ws[cutoff:]\n",
    "\n",
    "train_data = train_g + train_ws\n",
    "random.shuffle(train_data)\n",
    "\n",
    "test_data = test_g + test_ws\n",
    "random.shuffle(test_data)\n",
    "\n",
    "epochs = 8\n",
    "# Train the Model\n",
    "model = train_model(train_data,\n",
    "                    embed_dim,\n",
    "                    lstm_dim,\n",
    "                    hidden_dim,\n",
    "                    word2id,\n",
    "                    epochs,\n",
    "                    learning_rate)\n",
    "te = time()-t1\n",
    "message = \"Training finished in {:.4f} seconds, starting testing...\".format(te)\n",
    "print(message)\n",
    "print(\"...\")\n",
    "t1 = time()\n",
    "# Test the Model\n",
    "fold_results = test_model(test_data,model,word2id)\n",
    "result_list.append(fold_results)\n",
    "te = time()-t1\n",
    "message = \"Testing finished in {} seconds\".format(te)\n",
    "print(message)\n",
    "message = \"\\Accuracy is {}\".format(fold_results['accuracy'])\n",
    "print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_ws[0]\n",
    "\n",
    "def train_and_test (train_data,\n",
    "                    embed_dim,\n",
    "                    lstm_dim,\n",
    "                    hidden_dim,\n",
    "                    word2id,\n",
    "                    epochs,\n",
    "                    learning_rate,\n",
    "                    test_data):\n",
    "\n",
    "    voc_size = len(word2id)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    accuracy = test_model(test_data,linguo,word2id)[\"accuracy\"]\n",
    "    print(\"\\t Accuracy:{}\".format(accuracy))        \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word2id,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"Epoch{}:{}\".format(i,epoch_loss))\n",
    "        accuracy = test_model(test_data,linguo,word2id)[\"accuracy\"]\n",
    "        print(\"\\t Accuracy:{}\".format(accuracy))        \n",
    "                    \n",
    "    return linguo\n",
    "\n",
    "\n",
    "train_and_test (train_data,\n",
    "                    embed_dim,\n",
    "                    lstm_dim,\n",
    "                    hidden_dim,\n",
    "                    word2id,\n",
    "                    epochs,\n",
    "                    learning_rate,\n",
    "                    test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths_ws = [len(ws[0]) for ws in labeled_ws]\n",
    "lengths_g = [len(g[0]) for g in labeled_g]\n",
    "\n",
    "plt.hist(lengths_ws, bins, alpha=0.5, label='Word Salads',facecolor=\"r\")\n",
    "plt.hist(lengths_g, bins, alpha=0.5, label='Grammatical Sentences',facecolor=\"b\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Count')\n",
    "#plt.title('Sequence lengths for trigram word salads and corpus sentences')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n, bins, patches = plt.hist(lengths_g, 50, density=True, facecolor='g', alpha=0.75)\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Sentence lengths')\n",
    "plt.axis([8, 100, 0, 0.06])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error = [14045.571910912098,11181.830163920305,10130.082496052746,9347.070743460625,8630.008796883783,8012.440645607751,7472.498376756318,7084.894796584902]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(error)\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies = [0.5,0.7970137207425343,0.8153080441216034\n",
    ",0.825531342480495,0.824724239978477,0.8306429916599408,0.8216303470540759,0.8228410008071025\n",
    ",0.8140973903685768]\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.axis([0, 8, 0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
