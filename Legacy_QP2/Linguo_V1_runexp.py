# This model does the comparison between sentences and word salads
# Generated by drawing from the Unigram distribution

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# other utilities
import random
import sys
import re


def main():
    config_fn = "Data/" + sys.argv[1]
    # Load the config file
    corpus_path, experiments = load_config(config_fn)
    exp_tag = ".results." + config_fn.split(".")[-1]

    # Define filenames
    training_corpus_fn = corpus_path + ".labeled.training"
    testing_corpus_fn = corpus_path + ".labeled.testing"
    dict_fn = corpus_path + ".dict"
    results_fn = corpus_path + exp_tag

    # Load data
    training_instances = load_corpus(training_corpus_fn)
    test_instances = load_corpus(testing_corpus_fn)
    word_to_ix = load_dict(dict_fn)

    # Confirmation message
    print(
          '''Done loading data, you now have:
    {} train instances
    {} test instances:'''
          .format(len(training_instances),
                  len(test_instances)))

    # Open results file
    result_file = open(results_fn, "w")
    results = []
    header = '''-------------------------------------------------------
    This file includes results for experiments performed using
    model: Linguo_V1
    on the data: {corpus}
    with the configurations in {config}
    --------------------------------------------------------------------
    '''.format(corpus=corpus_path,
               config=config_fn)
    result_file.write(header)

    # Experiment loop
    ne = 1
    for exp_parameters in experiments:
        print("Experiment{num_exp}:".format(num_exp=ne))
        model, final_loss = train_model(exp_parameters,
                                        training_instances,
                                        word_to_ix)
        res = test_model(model, test_instances, word_to_ix)
        res["final_loss"] = final_loss
        results.append(res)
        save_summary(result_file, exp_parameters, res)
        ne += 1
    result_file.close()
# End of main


def train_model(exp_parameters,
                training_instances,
                word_to_ix):
    embed_dim = exp_parameters["embed_dim"]
    lstm_dim = exp_parameters["lstm_dim"]
    voc_size = len(word_to_ix)
    hidden_dim = exp_parameters["hidden_dim"]
    epochs = exp_parameters["epochs"]
    learning_rate = exp_parameters["learning_rate"]
    batch_size = exp_parameters["batch_size"]
    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim)
    optimizer = optim.SGD(linguo.parameters(), lr=learning_rate)
    loss_function = nn.NLLLoss()
    # Training time! Cue Eye of the Tiger
    for i in range(epochs):
        epoch_loss = 0
        random.shuffle(training_instances)
        fresh_instances = training_instances[:]
        # Get batches ready
        num_batches = math.floor(len(training_instances)/batch_size)
        batches = []
        for i in range(num_batches):
            batch_data = []
            batch_labels = []
            for j in range(batch_size):
                batch_data.append(prepare_input(word_to_ix, data))
        for data, label in training_instances:
            # Restart gradient
            linguo.zero_grad()

            # Run model
            in_sentence = prepare_input(word_to_ix, data)
            target = autograd.Variable(torch.LongTensor([label]))
            prediction = linguo(in_sentence)

            loss = loss_function(prediction, target)

            loss.backward()
            optimizer.step()
            # for parameter in linguo.parameters():
            #   parameter.data.sub_(parameter.grad.data*learning_rate)
            epoch_loss += loss.data[0]
        print("{}:{}".format(i, epoch_loss))
    return linguo, epoch_loss


def test_model(model, test_instances, word_to_ix):
    correct = 0
    results = {"fp": 0, "fn": 0, "tp": 0, "tn": 0}
    for testcase in test_instances:
        prepared_inputs = prepare_input(word_to_ix, testcase[0])
        tru_label = testcase[1]
        prediction_vec = model(prepared_inputs).view(2)
        # The system makes a forced choice, it choses the class with higher
        # probability (introduce confidence??)
        if prediction_vec.data[0] > prediction_vec.data[1]:
            prediction = 0
        else:
            prediction = 1
        # Tally the results
        if prediction == 1 and tru_label == 1:
            results["tp"] += 1
            correct += 1
        elif prediction == 0 and tru_label == 0:
            results["tn"] += 1
            correct += 1
        elif prediction == 0 and tru_label == 1:
            results["fn"] += 1
        else:
            results["fp"] += 1
    results["accuracy"] = correct/len(test_instances)
    return results


def save_summary(outfile, experiments, results):
    # Table with setting

    outtable = """"-----------------------------------------------------
Parameters:
-----------------
Embedding dimension: {embed}
LSTM dimension: {lstm}
Hidden Dimension: {hidden}
Number of Epochs: {epochs}
Batch Size: {batch_size}
Results:
--------
Final loss:{loss}
True Positives:{tp}
True Negatives:{tn}
False Positives:{fp}
False Negatives:{fn}
Accuracy:{accuracy}

    """.format(embed=experiments["embed_dim"],
               lstm=experiments["lstm_dim"],
               hidden=experiments["hidden_dim"],
               epochs=experiments["epochs"],
               batch_size=experiments["batch_size"],
               loss=results["final_loss"],
               accuracy=results["accuracy"],
               tp=results["tp"],
               tn=results["tn"],
               fp=results["fp"],
               fn=results["fn"],)
    print(outtable)
    outfile.write(outtable)


class Linguo(nn.Module):
    def __init__(self, embedding_dim, vocab_size, lstm_dim, hidden_dim):
        super(Linguo, self).__init__()
        # Store the hidden layer dimension
        self.hidden_dim = hidden_dim
        # Define word embeddings
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # Define LSTM
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        # Define hidden linear layer
        self.hidden2dec = nn.Linear(hidden_dim, 2)
        # Define the hidden state
        self.hstate = self.init_hstate()

    def forward(self, inputsentence):
        self.hstate = self.init_hstate()
        embeds = self.word_embeddings(inputsentence)
        lstm_out, self.hstate = self.lstm(
                                          embeds.view(len(inputsentence),
                                                      1, -1),
                                          self.hstate)
        decision_lin = self.hidden2dec(lstm_out[-1])
        decision_fin = F.log_softmax(decision_lin)
        return decision_fin

    def init_hstate(self):
        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))
        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))
        hidden_state = (var1, var2)
        return hidden_state


def prepare_input(word_to_ix, sentence):
    idxs = []
    for word in sentence:
        if word in word_to_ix:
            idxs.append(word_to_ix[word.lower()])
        else:
            idxs.append(word_to_ix["#unk"])
    tensor = torch.LongTensor(idxs)
    return autograd.Variable(tensor)


# Loads a file with the confuguration, check example config file
def load_config(config_path):
    int_parameters = ["embed_dim",
                      "lstm_dim",
                      "hidden_dim",
                      "epochs",
                      "batch_size"]
    float_parameters = ["learning_rate"]
    config_file = open(config_path, "r")
    line = config_file.readline()
    while line.strip() == "" or line[0] == "#":
        line = config_file.readline()
    par, value = re.split(r" *= *", line.strip())
    if par != "corpus_name":
        message = "Error in config file, corpus_name parameter not defined "
        terminate(message)
    else:
        corpus_name = "Data/" + value

    # Load for each experiment
    experiments = []
    currentExp = {}
    for line in config_file.readlines():
        if line.strip() != "" and line[0] != "#":
            if line.strip() == "/END":
                experiments.append(currentExp)
                currentExp = {}
            else:
                par, value = re.split(r" *= *", line.strip())
                if par in int_parameters:
                    currentExp[par] = int(value)
                elif par in float_parameters:
                    currentExp[par] = float(value)
    config_file.close()
    return corpus_name, experiments


# Loads from file the dictionary with the word ids
def load_dict(filename):
    infile = open(filename, "r")
    dictionary = {}
    for line in infile.readlines():
        line = line.strip()
        key, value = line.split("\:")
        dictionary[key] = int(value)
    infile.close()
    return dictionary


def load_corpus(filename):
    in_file = open(filename, "r")
    labeled_data = []
    for line in in_file.readlines():
        words_str, label = line.rstrip().split("|")
        words_list = words_str.split(" ")
        instance = [words_list, int(label)]
        labeled_data.append(instance)
    return labeled_data


def terminate(message):
    print("Program has encountered and error and must close")
    print(message)
    sys.exit()


main()
