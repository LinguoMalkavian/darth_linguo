{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses an LSTM with a single hidden layer and softmax activation to classify real versus corrupted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#spacy model\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "#load Spacy spanish model to handle tokenization of toy data\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Preprocessing data\n",
      "Uncorrupted data ready\n",
      "Corrupted data ready\n",
      "Training Corpus has 12363 labeled sentences.\n",
      "With 6503 real ones and 5860 ungrammatical ones\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_name = \"euro.toy\"\n",
    "\n",
    "test_file_base = \"Data/{cn}.{kind}.\".format(cn=corpus_name,kind = \"test\")\n",
    "train_file_base = \"Data/{cn}.{kind}.\".format(cn = corpus_name, kind = \"train\")\n",
    "\n",
    "print(\"Loading Data\")\n",
    "# Method to load corrupted data\n",
    "def load_corrupt(base_fn):\n",
    "    corrupt_types = [\"prepRM\", \"verbRM\", \"verbInfl\", \"adjInfl\"]\n",
    "    corrupt_sentences = {\"prepRM\":[],\n",
    "                         \"verbRM\":[],\n",
    "                         \"verbInfl\":[],\n",
    "                         \"adjInfl\":[]}\n",
    "    for corr_type in corrupt_types:\n",
    "        filename = \"{base}2corrupt.corrupted_by.{typ}\"\\\n",
    "        .format(\n",
    "            base=base_fn,\n",
    "            typ = corr_type)\n",
    "        file = open(filename,\"r\")\n",
    "        for line in file.readlines():\n",
    "            corrupt_sentences[corr_type].append(line.strip())\n",
    "    file.close()\n",
    "    return(corrupt_sentences)\n",
    "\n",
    "# Load corrupted train data\n",
    "corrupt_sentences_train = load_corrupt(train_file_base)\n",
    "    \n",
    "# Load uncorrupted train data\n",
    "uncorrupted_sentences_train = []\n",
    "uncorrupted_train_fn = train_file_base + \"2keep\"\n",
    "uncorrupted_train_file = open(uncorrupted_train_fn, \"r\")\n",
    "for line in uncorrupted_train_file.readlines():\n",
    "    uncorrupted_sentences_train.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "#Count to get rid of rare words\n",
    "counts = defaultdict(int)\n",
    "for sentence in uncorrupted_sentences_train:\n",
    "    tokens = sentence.split()\n",
    "    for token in sentence:\n",
    "        token = token.rstrip(\",!;:)*)»}\").lstrip(\"(«{\").lower()\n",
    "        counts[token] +=1\n",
    "        \n",
    "for corr_type in corrupt_sentences_train:\n",
    "    for sentence in corrupt_sentences_train[corr_type]:\n",
    "        tokens = sentence.split()\n",
    "        for token in sentence:\n",
    "            token = token.rstrip(\",!;:)*)»}\").lstrip(\"(«{\").lower()\n",
    "            counts[token] +=1\n",
    "            \n",
    "            \n",
    "# In case anyone else is reading this, I do know a \"hapax legomenoi\n",
    "# is a word that only apears once in a corpus but I want this\n",
    "# to be generalizable and realy like the variable name so here you\n",
    "# get to define the threshold\n",
    "\n",
    "hapax_threshold = 1            \n",
    "hapaxes = []\n",
    "counts[\"#unk\"]=0\n",
    "for key in counts:\n",
    "    if counts[key] <= hapax_threshold:\n",
    "        hapaxes.append(key)\n",
    "\n",
    "for hapax in hapaxes:\n",
    "    counts.pop(hapax)\n",
    "    \n",
    "# I know keys is kept as a set but I want it to remain a set\n",
    "vocabulary = set(counts.keys())\n",
    "vocabulary = vocabulary.union({\",\",\".\",\"(\",\")\",'''\"''',\";\",\":\",\"#num\",\n",
    "                              })\n",
    "\n",
    "\n",
    "def bulk_token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of sentences that have gone through the spacy pipeline\n",
    "    # (sentences have Doc type)\n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    # TODO: implement a version that replaces words by their tag instead\n",
    "    processed = [token_replacement(sentence,hapaxes) \n",
    "                 for sentence in sentences]\n",
    "    return processed\n",
    "\n",
    "def token_replacement(sentence,hapaxes):\n",
    "    number_regex = re.compile(\"\\d[.,0-9]+\")\n",
    "    parsed = nlp(sentence)\n",
    "    this_sentence = []\n",
    "    for token in parsed:\n",
    "        if number_regex.match(token.text):\n",
    "            this_sentence.append(\"#num\")\n",
    "        elif token.text.lower() in hapaxes:\n",
    "            pos = \"#\"+token.pos_.lower()\n",
    "            vocabulary.add(pos)\n",
    "            this_sentence.append(pos)\n",
    "        else:\n",
    "            this_sentence.append(token.text)\n",
    "    return this_sentence\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "print(\"Preprocessing data\")\n",
    "\n",
    "uncorrupted_sentences_train = bulk_token_replacement(\n",
    "    uncorrupted_sentences_train, hapaxes)\n",
    "print(\"Uncorrupted data ready\")\n",
    "\n",
    "for corr_type in corrupt_sentences_train:\n",
    "    corrupt_sentences_train[corr_type] = bulk_token_replacement(\n",
    "        corrupt_sentences_train[corr_type],\n",
    "        hapaxes)\n",
    "print(\"Corrupted data ready\")\n",
    "\n",
    "# Assign Labels and flags\n",
    "labeled_sentences_train = []\n",
    "for sentence in uncorrupted_sentences_train:\n",
    "    labeled_sentences_train.append((sentence,1,0))\n",
    "code = 0\n",
    "for corr_type in corrupt_sentences_train:\n",
    "    code += 1\n",
    "    for sentence in corrupt_sentences_train[corr_type]:\n",
    "        labeled_sentences_train.append((sentence,0,code))\n",
    "        \n",
    "#Shuffle\n",
    "\n",
    "random.shuffle(labeled_sentences_train)\n",
    "    \n",
    "# Save the corrupted corpora with labels\n",
    "training_corpus_fn = train_file_base + \"labeled.trainingV2\"\n",
    "\n",
    "def save_corpus(data,filename):\n",
    "    out_file = open(filename,\"w\")\n",
    "    for instance in data:\n",
    "        words = \" \".join(instance[0])\n",
    "        label = str(instance[1])\n",
    "        code = str(instance[2])\n",
    "        out = words + \"|\" + label + \"|\" + code + \"\\n\"\n",
    "        out_file.write(out)\n",
    "    out_file.close()\n",
    "    \n",
    "save_corpus(labeled_sentences_train,training_corpus_fn)\n",
    "\n",
    "\n",
    "# Print summary\n",
    "summary = \"\"\"Training Corpus has {full} labeled sentences.\n",
    "With {uncorrupted} real ones and {corrupted} ungrammatical ones\"\"\".format(\n",
    "            full = len(labeled_sentences_train),\n",
    "            uncorrupted =len(uncorrupted_sentences_train),\n",
    "            corrupted= len(labeled_sentences_train)-len(uncorrupted_sentences_train)\n",
    "            )\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Build the identifiers for all words in the training vocabulary\n",
    "word_to_ix = {}\n",
    "for word in vocabulary:\n",
    "    word_to_ix[word]= len(word_to_ix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lo',\n",
       " 'mucho',\n",
       " ',',\n",
       " 'señor',\n",
       " 'Hänsch',\n",
       " ',',\n",
       " 'señor',\n",
       " 'Cox',\n",
       " ',',\n",
       " 'no',\n",
       " 'he',\n",
       " 'advertido',\n",
       " 'que',\n",
       " 'ustedes',\n",
       " 'pedían',\n",
       " 'la',\n",
       " 'palabra',\n",
       " '.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_sentences_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural network basic architecture\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        linguo.hstate = linguo.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence,nlp):\n",
    "    idxs = []\n",
    "    try:\n",
    "        #If every word is known then our work is easy\n",
    "        for word in sentence:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "    except KeyError :\n",
    "        #If there is at least one unknown word we parse and tag\n",
    "        processed = nlp(\" \".join(sentence))\n",
    "        for token in processed:\n",
    "            word = token.text.lower()\n",
    "            pos = \"#\"+token.pos_.lower()\n",
    "            if word in word_to_ix:\n",
    "                #Known tokens are still input normally\n",
    "                idxs.append(word_to_ix[word])\n",
    "            elif pos in word_to_ix:\n",
    "                #Tagged inputs are replaced by their tag\n",
    "                idxs.append(word_to_ix[pos])\n",
    "            else:\n",
    "                # If no tad is available we default to a proper noun\n",
    "                idxs.append(word_to_ix[\"#propn\"])\n",
    "\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:8720.684995666146\n",
      "1:8715.43458160758\n",
      "2:8714.38810968399\n",
      "3:8715.524949222803\n",
      "4:8717.333831340075\n",
      "5:8726.007983982563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-9cff792ce62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Run model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0min_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinguo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-3e03b66a96da>\u001b[0m in \u001b[0;36mprepare_input\u001b[0;34m(word_to_ix, sentence, nlp)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#If there is at least one unknown word we parse and tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad),\n\u001b[0;32m--> 280\u001b[0;31m                                          drop=drop)\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcontinue_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "#Modify hyper parameters here while we get config files\n",
    "embed_dim = 32\n",
    "lstm_dim = 64\n",
    "voc_size = len(word_to_ix)\n",
    "hidden_dim = 64\n",
    "epochs = 25\n",
    "linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "optimizer = optim.SGD(linguo.parameters(),lr=0.1)\n",
    "loss_function = nn.NLLLoss()\n",
    "learning_rate=0.1\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(labeled_sentences_train)\n",
    "    for data, label , code in labeled_sentences_train:\n",
    "        # Restart gradient\n",
    "        linguo.zero_grad()\n",
    "        \n",
    "        # Run model\n",
    "        in_sentence = prepare_input(word_to_ix,data,nlp)\n",
    "        target = autograd.Variable(torch.LongTensor([label]))\n",
    "        prediction = linguo(in_sentence)\n",
    "        #Calculate loss and backpropagate\n",
    "        \n",
    "        #Squared Loss\n",
    "        #loss = torch.pow(target-prediction.view(1),2)\n",
    "        loss = loss_function(prediction,target) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #for parameter in linguo.parameters():\n",
    "        #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "        epoch_loss += loss.data[0]\n",
    "    print(\"{}:{}\".format(i,epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'el'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-317525c239c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_to_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"yo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"el\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'el'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
