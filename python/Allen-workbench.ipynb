{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers.token import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LinguoDatasetReader(DatasetReader):\n",
    "    \"\"\"Dataset reader for preprocessed sentences (tokens separated by spaces) \"\"\"\n",
    "    GRAMMATICALITY_labels = [\"ungrammatical\",\"grammatical\"]\n",
    "    UG_TYPE_labels = [\"WS\",\"VA\",\"AA\",\"RV\",\"G\"]\n",
    "    \n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    \n",
    "    def text_to_instance(self, line):\n",
    "        elements = line.strip().split()\n",
    "        label = self.GRAMMATICALITY_labels[int(elements[0])]\n",
    "        ugType = elements[1]\n",
    "        sentence = elements [2:]\n",
    "        sentence_field = TextField(tokens,self.token_indexers)\n",
    "        fields = {\"sentence\":sentence_field}\n",
    "        if glabel:\n",
    "            glabel_field = LabelField(label=glabel,label_namespace = \"grammaticality_labels\")\n",
    "            fields[\"g_label\"] = glabel_field\n",
    "        if ugType:\n",
    "            ugType_field = LabelField(label=ugType, label_namespace = \"ugtype_labels\")\n",
    "            fields[\"ug_type\"] = ugType_field\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path:str, label:str=None, ugType:str=None) -> Iterator[Instance]:\n",
    "        with open(file_path) as infile:\n",
    "            for line in infile:\n",
    "                ield self.text_to_instance([Token(word) for word in sentence],label,ugType)\n",
    "                \n",
    "class AllenLinguo(Model):\n",
    "    \n",
    "    def __init__(self,word_embeddings : TextFieldEmbedder,\n",
    "                encoder : Seq2VecEncoder,\n",
    "                vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.hidden2decision = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                              out_features=vocab.get_vocab_size(\"grammaticality_labels\"))\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "               sentence: Dict[str, torch.Tensor],\n",
    "               g_label: torch.Tensor = None,\n",
    "               ug_type: torch.Tensor = None) -> torch.Tensor:\n",
    "        \n",
    "        mask = get_text_field_mask(sentence)\n",
    "        \n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        \n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        \n",
    "        tag_logits = self.hidden2decision(encoder_out)\n",
    "        \n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        \n",
    "        if g_label is not None:\n",
    "            self.accuracy(tag_logits, g_label)\n",
    "            #print(tag_logits)\n",
    "            output[\"loss\"] = self.loss_function(tag_logits, g_label)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}    \n",
    "            \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 7805.67it/s]\n",
      "371it [00:00, 14350.41it/s]\n",
      "100%|██████████| 1483/1483 [00:00<00:00, 46955.54it/s]\n"
     ]
    }
   ],
   "source": [
    "training_fn = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/toy_corpus/toy_training-GvsWS\"\n",
    "testing_fn = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/toy_corpus/toy_testing-GvsWS\"\n",
    "\n",
    "reader = LinguoDatasetReader()\n",
    "\n",
    "train_dataset = reader.read(training_fn)\n",
    "validation_dataset = reader.read(testing_fn)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset,min_count={'tokens': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: <unk>\t\tFrequency: 3238\n",
      "\tToken: de\t\tFrequency: 2976\n",
      "\tToken: ,\t\tFrequency: 2401\n",
      "\tToken: la\t\tFrequency: 1996\n",
      "\tToken: que\t\tFrequency: 1577\n",
      "\tToken: .\t\tFrequency: 1534\n",
      "\tToken: <eos>\t\tFrequency: 1483\n",
      "\tToken: en\t\tFrequency: 1284\n",
      "\tToken: el\t\tFrequency: 1136\n",
      "\tToken: y\t\tFrequency: 1074\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: extraordinariamente\t\tlength: 19\tFrequency: 5\n",
      "\tToken: reestructuraciones\t\tlength: 18\tFrequency: 8\n",
      "\tToken: telecomunicaciones\t\tlength: 18\tFrequency: 2\n",
      "\tToken: Schleswig-Holstein\t\tlength: 18\tFrequency: 1\n",
      "\tToken: autodiscriminación\t\tlength: 18\tFrequency: 1\n",
      "\tToken: concentracionarios\t\tlength: 18\tFrequency: 1\n",
      "\tToken: responsabilizarnos\t\tlength: 18\tFrequency: 1\n",
      "\tToken: responsabilidades\t\tlength: 17\tFrequency: 7\n",
      "\tToken: renacionalización\t\tlength: 17\tFrequency: 1\n",
      "\tToken: contraproducentes\t\tlength: 17\tFrequency: 1\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: !\t\tlength: 1\tFrequency: 1\n",
      "\tToken: 9\t\tlength: 1\tFrequency: 1\n",
      "\tToken: 3\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ?\t\tlength: 1\tFrequency: 1\n",
      "\tToken: u\t\tlength: 1\tFrequency: 2\n",
      "\tToken: 6\t\tlength: 1\tFrequency: 3\n",
      "\tToken: 8\t\tlength: 1\tFrequency: 3\n",
      "\tToken: 4\t\tlength: 1\tFrequency: 6\n",
      "\tToken: 2\t\tlength: 1\tFrequency: 7\n",
      "\tToken: Y\t\tlength: 1\tFrequency: 9\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'grammaticality_labels':\n",
      "\tToken: ungrammatical\t\tFrequency: 744\n",
      "\tToken: grammatical\t\tFrequency: 739\n",
      "\n",
      "Top 10 longest tokens in namespace 'grammaticality_labels':\n",
      "\tToken: ungrammatical\t\tlength: 13\tFrequency: 744\n",
      "\tToken: grammatical\t\tlength: 11\tFrequency: 739\n",
      "\n",
      "Top 10 shortest tokens in namespace 'grammaticality_labels':\n",
      "\tToken: grammatical\t\tlength: 11\tFrequency: 739\n",
      "\tToken: ungrammatical\t\tlength: 13\tFrequency: 744\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'ugtype_labels':\n",
      "\tToken: WS\t\tFrequency: 744\n",
      "\tToken: G\t\tFrequency: 739\n",
      "\n",
      "Top 10 longest tokens in namespace 'ugtype_labels':\n",
      "\tToken: WS\t\tlength: 2\tFrequency: 744\n",
      "\tToken: G\t\tlength: 1\tFrequency: 739\n",
      "\n",
      "Top 10 shortest tokens in namespace 'ugtype_labels':\n",
      "\tToken: G\t\tlength: 1\tFrequency: 739\n",
      "\tToken: WS\t\tlength: 2\tFrequency: 744\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4909, loss: 0.7008 ||: 100%|██████████| 742/742 [00:05<00:00, 125.01it/s]\n",
      "accuracy: 0.4987, loss: 0.6931 ||: 100%|██████████| 186/186 [00:00<00:00, 389.90it/s]\n",
      "accuracy: 0.4680, loss: 0.7032 ||: 100%|██████████| 742/742 [00:05<00:00, 129.34it/s]\n",
      "accuracy: 0.4987, loss: 0.7008 ||: 100%|██████████| 186/186 [00:00<00:00, 394.13it/s]\n",
      "accuracy: 0.4909, loss: 0.7015 ||: 100%|██████████| 742/742 [00:05<00:00, 132.34it/s]\n",
      "accuracy: 0.4987, loss: 0.6971 ||: 100%|██████████| 186/186 [00:00<00:00, 397.00it/s]\n",
      "accuracy: 0.5253, loss: 0.6982 ||: 100%|██████████| 742/742 [00:05<00:00, 129.51it/s]\n",
      "accuracy: 0.4987, loss: 0.6904 ||: 100%|██████████| 186/186 [00:00<00:00, 393.10it/s]\n",
      "accuracy: 0.6096, loss: 0.6383 ||: 100%|██████████| 742/742 [00:05<00:00, 129.84it/s]\n",
      "accuracy: 0.5013, loss: 1.6901 ||: 100%|██████████| 186/186 [00:00<00:00, 392.06it/s]\n",
      "accuracy: 0.8901, loss: 0.2705 ||: 100%|██████████| 742/742 [00:05<00:00, 126.32it/s]\n",
      "accuracy: 0.9677, loss: 0.1229 ||: 100%|██████████| 186/186 [00:00<00:00, 400.38it/s]\n",
      "accuracy: 0.9582, loss: 0.1484 ||: 100%|██████████| 742/742 [00:05<00:00, 132.02it/s]\n",
      "accuracy: 0.9677, loss: 0.1106 ||: 100%|██████████| 186/186 [00:00<00:00, 398.02it/s]\n",
      "accuracy: 0.7653, loss: 0.4046 ||: 100%|██████████| 742/742 [00:05<00:00, 127.95it/s]\n",
      "accuracy: 0.5013, loss: 0.6817 ||: 100%|██████████| 186/186 [00:00<00:00, 395.54it/s]\n",
      "accuracy: 0.8739, loss: 0.2726 ||: 100%|██████████| 742/742 [00:05<00:00, 125.85it/s]\n",
      "accuracy: 0.9677, loss: 0.1085 ||: 100%|██████████| 186/186 [00:00<00:00, 394.98it/s]\n",
      "accuracy: 0.9568, loss: 0.1477 ||: 100%|██████████| 742/742 [00:05<00:00, 127.54it/s]\n",
      "accuracy: 0.9542, loss: 0.1648 ||: 100%|██████████| 186/186 [00:00<00:00, 394.56it/s]\n",
      "accuracy: 0.9501, loss: 0.1528 ||: 100%|██████████| 742/742 [00:05<00:00, 131.46it/s]\n",
      "accuracy: 0.9730, loss: 0.0929 ||: 100%|██████████| 186/186 [00:00<00:00, 395.36it/s]\n",
      "accuracy: 0.9710, loss: 0.0931 ||: 100%|██████████| 742/742 [00:05<00:00, 130.64it/s]\n",
      "accuracy: 0.9946, loss: 0.0392 ||: 100%|██████████| 186/186 [00:00<00:00, 390.84it/s]\n",
      "accuracy: 0.9933, loss: 0.0238 ||: 100%|██████████| 742/742 [00:05<00:00, 131.76it/s]\n",
      "accuracy: 0.9973, loss: 0.0205 ||: 100%|██████████| 186/186 [00:00<00:00, 381.96it/s]\n",
      "accuracy: 0.9980, loss: 0.0126 ||: 100%|██████████| 742/742 [00:05<00:00, 131.35it/s]\n",
      "accuracy: 0.9973, loss: 0.0192 ||: 100%|██████████| 186/186 [00:00<00:00, 392.69it/s]\n",
      "accuracy: 0.9987, loss: 0.0088 ||: 100%|██████████| 742/742 [00:05<00:00, 131.18it/s]\n",
      "accuracy: 0.9973, loss: 0.0188 ||: 100%|██████████| 186/186 [00:00<00:00, 392.26it/s]\n",
      "accuracy: 0.9993, loss: 0.0061 ||: 100%|██████████| 742/742 [00:05<00:00, 130.10it/s]\n",
      "accuracy: 0.9973, loss: 0.0193 ||: 100%|██████████| 186/186 [00:00<00:00, 390.57it/s]\n",
      "accuracy: 0.9993, loss: 0.0058 ||: 100%|██████████| 742/742 [00:05<00:00, 130.02it/s]\n",
      "accuracy: 0.9973, loss: 0.0194 ||: 100%|██████████| 186/186 [00:00<00:00, 385.52it/s]\n",
      "accuracy: 0.9993, loss: 0.0058 ||: 100%|██████████| 742/742 [00:05<00:00, 129.89it/s]\n",
      "accuracy: 0.9973, loss: 0.0194 ||: 100%|██████████| 186/186 [00:00<00:00, 390.32it/s]\n",
      "accuracy: 0.9993, loss: 0.0057 ||: 100%|██████████| 742/742 [00:05<00:00, 139.14it/s]\n",
      "accuracy: 0.9973, loss: 0.0187 ||: 100%|██████████| 186/186 [00:00<00:00, 389.19it/s]\n",
      "accuracy: 0.9993, loss: 0.0057 ||: 100%|██████████| 742/742 [00:05<00:00, 129.09it/s]\n",
      "accuracy: 0.9973, loss: 0.0189 ||: 100%|██████████| 186/186 [00:00<00:00, 389.97it/s]\n",
      "accuracy: 0.9993, loss: 0.0056 ||: 100%|██████████| 742/742 [00:05<00:00, 128.46it/s]\n",
      "accuracy: 0.9973, loss: 0.0188 ||: 100%|██████████| 186/186 [00:00<00:00, 388.88it/s]\n",
      "accuracy: 0.9993, loss: 0.0056 ||: 100%|██████████| 742/742 [00:05<00:00, 144.61it/s]\n",
      "accuracy: 0.9973, loss: 0.0182 ||: 100%|██████████| 186/186 [00:00<00:00, 389.46it/s]\n",
      "accuracy: 0.9993, loss: 0.0056 ||: 100%|██████████| 742/742 [00:05<00:00, 129.94it/s]\n",
      "accuracy: 0.9973, loss: 0.0179 ||: 100%|██████████| 186/186 [00:00<00:00, 388.66it/s]\n",
      "accuracy: 0.9993, loss: 0.0056 ||: 100%|██████████| 742/742 [00:05<00:00, 130.11it/s]\n",
      "accuracy: 0.9973, loss: 0.0188 ||: 100%|██████████| 186/186 [00:00<00:00, 390.24it/s]\n",
      "accuracy: 0.9993, loss: 0.0056 ||: 100%|██████████| 742/742 [00:05<00:00, 129.90it/s]\n",
      "accuracy: 0.9973, loss: 0.0185 ||: 100%|██████████| 186/186 [00:00<00:00, 389.09it/s]\n",
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/envs/allen/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/envs/allen/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = AllenLinguo(word_embeddings, lstm, vocab)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=25)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "\n",
    "# tag_logits = predictor.predict(\"La proxima vez no habrá otra opción\")['tag_logits']\n",
    "\n",
    "#tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n",
    "# print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag_logits = predictor.predict(\"La proxima vez no habrá otra opción\")['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_files(\"/Users/pablo/Dropbox/workspace/darth_linguo/results/TEST2/vocabulary\")\n",
    "tag_logits = torch.tensor([[ 2, 1],[ 1, 2],[ 2, 1],[ 2, 1],[0, 1],[1,0],[1,0],[1,0],[1,0],[0,1],[0,1]])\n",
    "g_label = torch.tensor([1,1,0,0,0,0,0,0,0,0,0])\n",
    "ug_type = torch.tensor([0,0,1,1,1,2,2,3,3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:0.5\n",
      "WS:0.6666666666666666\n",
      "RV:1.0\n",
      "AA:0.5\n",
      "VA:0.0\n"
     ]
    }
   ],
   "source": [
    "specific_gold = {n:[] for n in range(vocab.get_vocab_size(namespace=\"ugtype_labels\"))}\n",
    "specific_pred = {n:[] for n in range(vocab.get_vocab_size(namespace=\"ugtype_labels\"))}\n",
    "specificAccuracies = {n: CategoricalAccuracy() for n in range(vocab.get_vocab_size(namespace=\"ugtype_labels\"))}\n",
    "for ind in range(len(g_label)):\n",
    "    g_lab = g_label[ind].item()\n",
    "    logit = [ tag_logits[ind][0].item(), tag_logits[ind][1].item() ]\n",
    "    spec_label = ug_type[ind].item()\n",
    "    specific_gold[spec_label].append(g_lab)\n",
    "    specific_pred[spec_label].append(logit)\n",
    "    \n",
    "for ind in specificAccuracies:\n",
    "    if specific_pred[ind]:\n",
    "        preds = torch.tensor(specific_pred[ind])\n",
    "        labels = torch.tensor(specific_gold[ind])\n",
    "        specificAccuracies[ind](preds,labels)\n",
    "    name = vocab.get_token_from_index(ind, namespace=\"ugtype_labels\")\n",
    "    accuracy = specificAccuracies[ind].get_metric()\n",
    "    print(\"{}:{}\".format(name,accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_accuracy(torch.tensor(specific_pred[2]), torch.tensor(specific_gold[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_accuracy.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allen]",
   "language": "python",
   "name": "conda-env-allen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
