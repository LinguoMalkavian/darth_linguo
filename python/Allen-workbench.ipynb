{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers.token import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LinguoDatasetReader(DatasetReader):\n",
    "    \"\"\"Dataset reader for preprocessed sentences (tokens separated by spaces) \"\"\"\n",
    "    GRAMMATICALITY_labels = [\"ungrammatical\",\"grammatical\"]\n",
    "    UG_TYPE_labels = [\"WS\",\"VA\",\"AA\",\"RV\",\"G\"]\n",
    "    \n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    \n",
    "    def text_to_instance(self, tokens:List[Token], glabel:int=None, ugType:str=None ):\n",
    "        sentence_field = TextField(tokens,self.token_indexers)\n",
    "        fields = {\"sentence\":sentence_field}\n",
    "        if glabel:\n",
    "            glabel_field = LabelField(label=glabel,label_namespace = \"grammaticality_labels\")\n",
    "            fields[\"g_label\"] = glabel_field\n",
    "        if ugType:\n",
    "            ugType_field = LabelField(label=ugType, label_namespace = \"ugtype_labels\")\n",
    "            fields[\"ug_type\"] = ugType_field\n",
    "        return Instance(fields)\n",
    "    def _read(self, file_path:str, label:str=None, ugType:str=None) -> Iterator[Instance]:\n",
    "        with open(file_path) as infile:\n",
    "            for line in infile:\n",
    "                elements = line.strip().split()\n",
    "                label = self.GRAMMATICALITY_labels[int(elements[0])]\n",
    "                if label == self.GRAMMATICALITY_labels[0]:\n",
    "                    ugType = elements[1]\n",
    "                else :\n",
    "                    ugType = \"G\"\n",
    "                sentence = elements [2:]\n",
    "                yield self.text_to_instance([Token(word) for word in sentence],label,ugType)\n",
    "                \n",
    "class AllenLinguo(Model):\n",
    "    \n",
    "    def __init__(self,word_embeddings : TextFieldEmbedder,\n",
    "                encoder : Seq2VecEncoder,\n",
    "                vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.hidden2decision = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                              out_features=vocab.get_vocab_size(\"grammaticality_labels\"))\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "               sentence: Dict[str, torch.Tensor],\n",
    "               g_label: torch.Tensor = None,\n",
    "               ug_type: torch.Tensor = None) -> torch.Tensor:\n",
    "        \n",
    "        mask = get_text_field_mask(sentence)\n",
    "        \n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        \n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        \n",
    "        tag_logits = self.hidden2decision(encoder_out)\n",
    "        \n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        \n",
    "        if g_label is not None:\n",
    "            self.accuracy(tag_logits, g_label)\n",
    "            #print(tag_logits)\n",
    "            output[\"loss\"] = self.loss_function(tag_logits, g_label)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}    \n",
    "            \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1483it [00:00, 11193.97it/s]\n",
      "371it [00:00, 18373.70it/s]\n",
      "100%|██████████| 1854/1854 [00:00<00:00, 50295.84it/s]\n"
     ]
    }
   ],
   "source": [
    "training_fn = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/toy_corpus/toy_training-GvsWS\"\n",
    "testing_fn = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/toy_corpus/toy_testing-GvsWS\"\n",
    "\n",
    "reader = LinguoDatasetReader()\n",
    "\n",
    "train_dataset = reader.read(training_fn)\n",
    "validation_dataset = reader.read(testing_fn)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextField of length 86 with text: \n",
      " \t\t[en, este, parlamento, en, el, marco, en, el, ámbito, de, los, países, de, europa, central, y,\n",
      "\t\toriental, ;, un, capítulo, especial, de, su, coordinación, ,, es, decir, ,, la, <unk>, <unk>, los,\n",
      "\t\taspectos, <unk>, regionales, y, locales, de, los, fondos, estructurales, y, su, <unk>, en, que, no,\n",
      "\t\than, realizado, <unk>, <unk>, ,, la, comisión, <unk>, <unk>, de, <unk>, <unk>, de, nuevo, ,, pues,\n",
      "\t\t,, necesario, <unk>, esa, flexibilidad, de, los, datos, y, ,, claro, está, ,, sin, embargo, ,,\n",
      "\t\t<unk>, <unk>, en, la, comisión, ., <eos>]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "LabelField with label: ungrammatical in namespace: 'grammaticality_labels'.'\n"
     ]
    }
   ],
   "source": [
    "print(validation_dataset[1].fields[\"sentence\"])\n",
    "print(validation_dataset[1].fields[\"g_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4983, loss: 0.7023 ||: 100%|██████████| 742/742 [00:06<00:00, 119.94it/s]\n",
      "accuracy: 0.5013, loss: 0.6936 ||: 100%|██████████| 186/186 [00:00<00:00, 360.23it/s]\n",
      "accuracy: 0.4976, loss: 0.7013 ||: 100%|██████████| 742/742 [00:06<00:00, 122.03it/s]\n",
      "accuracy: 0.5013, loss: 0.6940 ||: 100%|██████████| 186/186 [00:00<00:00, 387.66it/s]\n",
      "accuracy: 0.4990, loss: 0.7003 ||: 100%|██████████| 742/742 [00:06<00:00, 122.44it/s]\n",
      "accuracy: 0.5013, loss: 0.6946 ||: 100%|██████████| 186/186 [00:00<00:00, 377.17it/s]\n",
      "accuracy: 0.5051, loss: 0.6998 ||: 100%|██████████| 742/742 [00:06<00:00, 121.21it/s]\n",
      "accuracy: 0.4987, loss: 0.6944 ||: 100%|██████████| 186/186 [00:00<00:00, 383.68it/s]\n",
      "accuracy: 0.6298, loss: 0.6084 ||: 100%|██████████| 742/742 [00:06<00:00, 134.98it/s]\n",
      "accuracy: 0.5013, loss: 0.6921 ||: 100%|██████████| 186/186 [00:00<00:00, 383.97it/s]\n",
      "accuracy: 0.8247, loss: 0.3583 ||: 100%|██████████| 742/742 [00:06<00:00, 118.84it/s]\n",
      "accuracy: 0.9677, loss: 0.1199 ||: 100%|██████████| 186/186 [00:00<00:00, 377.22it/s]\n",
      "accuracy: 0.9582, loss: 0.1506 ||: 100%|██████████| 742/742 [00:06<00:00, 119.52it/s]\n",
      "accuracy: 0.9704, loss: 0.1132 ||: 100%|██████████| 186/186 [00:00<00:00, 376.28it/s]\n",
      "accuracy: 0.9595, loss: 0.1448 ||: 100%|██████████| 742/742 [00:06<00:00, 116.33it/s]\n",
      "accuracy: 0.9704, loss: 0.1302 ||: 100%|██████████| 186/186 [00:00<00:00, 375.75it/s]\n",
      "accuracy: 0.9589, loss: 0.1375 ||: 100%|██████████| 742/742 [00:06<00:00, 118.38it/s]\n",
      "accuracy: 0.9704, loss: 0.1045 ||: 100%|██████████| 186/186 [00:00<00:00, 371.38it/s]\n",
      "accuracy: 0.9717, loss: 0.1032 ||: 100%|██████████| 742/742 [00:06<00:00, 112.87it/s]\n",
      "accuracy: 0.9704, loss: 0.0819 ||: 100%|██████████| 186/186 [00:00<00:00, 373.65it/s]\n",
      "accuracy: 0.9757, loss: 0.0986 ||: 100%|██████████| 742/742 [00:06<00:00, 112.57it/s]\n",
      "accuracy: 0.9865, loss: 0.0502 ||: 100%|██████████| 186/186 [00:00<00:00, 374.54it/s]\n",
      "accuracy: 0.6993, loss: 0.4958 ||: 100%|██████████| 742/742 [00:06<00:00, 117.33it/s]\n",
      "accuracy: 0.6792, loss: 0.6629 ||: 100%|██████████| 186/186 [00:00<00:00, 369.68it/s]\n",
      "accuracy: 0.8179, loss: 0.3891 ||: 100%|██████████| 742/742 [00:06<00:00, 115.78it/s]\n",
      "accuracy: 0.9596, loss: 0.1376 ||: 100%|██████████| 186/186 [00:00<00:00, 364.46it/s]\n",
      "accuracy: 0.9400, loss: 0.1754 ||: 100%|██████████| 742/742 [00:06<00:00, 116.35it/s]\n",
      "accuracy: 0.9811, loss: 0.0811 ||: 100%|██████████| 186/186 [00:00<00:00, 367.51it/s]\n",
      "accuracy: 0.9791, loss: 0.0759 ||: 100%|██████████| 742/742 [00:06<00:00, 110.32it/s]\n",
      "accuracy: 0.9677, loss: 0.0887 ||: 100%|██████████| 186/186 [00:00<00:00, 359.41it/s]\n",
      "accuracy: 0.9771, loss: 0.0905 ||: 100%|██████████| 742/742 [00:06<00:00, 111.18it/s]\n",
      "accuracy: 0.9865, loss: 0.0616 ||: 100%|██████████| 186/186 [00:00<00:00, 344.50it/s]\n",
      "accuracy: 0.9926, loss: 0.0365 ||: 100%|██████████| 742/742 [00:07<00:00, 111.66it/s]\n",
      "accuracy: 0.9838, loss: 0.0678 ||: 100%|██████████| 186/186 [00:00<00:00, 326.98it/s]\n",
      "accuracy: 0.9926, loss: 0.0338 ||: 100%|██████████| 742/742 [00:06<00:00, 109.89it/s]\n",
      "accuracy: 0.8733, loss: 0.6406 ||: 100%|██████████| 186/186 [00:00<00:00, 336.12it/s]\n",
      "accuracy: 0.9083, loss: 0.1787 ||: 100%|██████████| 742/742 [00:07<00:00, 105.26it/s]\n",
      "accuracy: 0.7817, loss: 0.5415 ||: 100%|██████████| 186/186 [00:00<00:00, 294.80it/s]\n",
      "accuracy: 0.8456, loss: 0.3395 ||: 100%|██████████| 742/742 [00:06<00:00, 109.40it/s]\n",
      "accuracy: 0.9030, loss: 0.2446 ||: 100%|██████████| 186/186 [00:00<00:00, 349.95it/s]\n",
      "accuracy: 0.9332, loss: 0.1743 ||: 100%|██████████| 742/742 [00:06<00:00, 111.30it/s]\n",
      "accuracy: 0.9811, loss: 0.0554 ||: 100%|██████████| 186/186 [00:00<00:00, 335.99it/s]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = AllenLinguo(word_embeddings, lstm, vocab)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=25)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "\n",
    "# tag_logits = predictor.predict(\"La proxima vez no habrá otra opción\")['tag_logits']\n",
    "\n",
    "#tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n",
    "# print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tag_logits = predictor.predict(\"La proxima vez no habrá otra opción\")['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
