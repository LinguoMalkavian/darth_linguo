{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "template_fn = os.path.abspath(\"../experiments/exp_1_no-WS_350-1000.json\")\n",
    "\n",
    "def save_config(filename, config):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(config, outfile, indent=4)\n",
    "\n",
    "def load_template(filename):\n",
    "    with open(template_fn, \"r\") as template_file:\n",
    "        config = json.loads(template_file.read())\n",
    "    return config\n",
    "\n",
    "def build_bash(bash_fn ,config_list):\n",
    "    header = \"\"\"#Run the experiments\n",
    "    set -x\n",
    "    source activate allennlp\n",
    "    \"\"\"\n",
    "    single_exp = \"\"\"allennlp train {conf} -s {serial} --include-package allen_linguo\n",
    "    \"\"\"\n",
    "    serial_base = os.path.abspath(\"../results\")\n",
    "    with open(bash_fn, \"w\") as bash_file:\n",
    "        bash_file.write(header)\n",
    "        for conf_path in config_list:\n",
    "            exp_name = os.path.basename(conf_path).replace(\".json\",\"\")\n",
    "            serial = os.path.join(serial_base,exp_name)\n",
    "            bash_file.write(single_exp.format(conf=conf_path, serial=serial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "base_fn = os.path.abspath(\"../experiments/\")\n",
    "base_fn = os.path.join(base_fn, \"exp-1.1_no-WS_{}-{}.json\")\n",
    "exp_files = []\n",
    "while dim < 1024:\n",
    "    config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = dim\n",
    "    config[\"model\"][\"encoder\"][\"input_size\"] = dim\n",
    "    config[\"model\"][\"encoder\"][\"hidden_size\"] = dim\n",
    "    filename = base_fn.format(dim,dim)\n",
    "    exp_files.append(filename)\n",
    "    save_config(filename, config)\n",
    "    dim = dim*2\n",
    "    \n",
    "config[\"model\"][\"encoder\"][\"hidden_size\"] = 1024\n",
    "filename = base_fn.format(512,1024)\n",
    "exp_files.append(filename)\n",
    "save_config(filename, config)\n",
    "\n",
    "bash_fn = os.path.abspath(\"../pbs_scripts/run_all-1.1.sh\")\n",
    "\n",
    "build_bash(bash_fn, exp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "base_fn = os.path.abspath(\"../experiments/\")\n",
    "base_fn = os.path.join(base_fn, \"exp-1.1_no-WS_{}-{}.json\")\n",
    "exp_files = []\n",
    "while dim < 1024:\n",
    "    config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = dim\n",
    "    config[\"model\"][\"encoder\"][\"input_size\"] = dim\n",
    "    config[\"model\"][\"encoder\"][\"hidden_size\"] = dim\n",
    "    filename = base_fn.format(dim,dim)\n",
    "    exp_files.append(filename)\n",
    "    save_config(filename, config)\n",
    "    dim = dim*2\n",
    "    \n",
    "config[\"model\"][\"encoder\"][\"hidden_size\"] = 1024\n",
    "filename = base_fn.format(512,1024)\n",
    "exp_files.append(filename)\n",
    "save_config(filename, config)\n",
    "\n",
    "bash_fn = os.path.abspath(\"../pbs_scripts/run_all-1.1.sh\")\n",
    "\n",
    "build_bash(bash_fn, exp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_template(template_fn)\n",
    "\n",
    "#For exp 1.2\n",
    "exp_list = []\n",
    "# For random initialization \n",
    "config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = 300\n",
    "config[\"model\"][\"encoder\"][\"hidden_size\"] = 512\n",
    "config[\"model\"][\"encoder\"][\"input_size\"] = 300\n",
    "random_fn = os.path.abspath(\"../experiments/exp-1.2_random.json\")\n",
    "exp_list.append(random_fn)\n",
    "save_config(random_fn, config)\n",
    "\n",
    "# For pretrained with refinement\n",
    "config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"pretrained_file\"] =  \"/home/lab/Pablo/darth_linguo/resources/SBW-vectors-300-min5.txt\"\n",
    "config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"trainable\"] = True\n",
    "pretrained_cont_fn = os.path.abspath(\"../experiments/exp-1.2_pretrained-cont.json\")\n",
    "exp_list.append(pretrained_cont_fn)\n",
    "save_config(pretrained_cont_fn, config)\n",
    "\n",
    "#Pretrainde with frozen weights\n",
    "pretrained_freeze_fn = os.path.abspath(\"../experiments/exp-1.2_pretrained-freeze.json\")\n",
    "config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"trainable\"] = False\n",
    "exp_list.append(pretrained_freeze_fn)\n",
    "save_config(pretrained_freeze_fn, config)\n",
    "\n",
    "script_fn = os.path.abspath(\"../pbs_scripts/run_all-1.2.sh\")\n",
    "\n",
    "build_bash(script_fn, exp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For experiment 2 (varying encoder depth and directionality)\n",
    "\n",
    "config = load_template(template_fn)\n",
    "exp_list = []\n",
    "base_filename = os.path.abspath(\"../experiments/exp-2.1_{}.json\") \n",
    "# common settings\n",
    "config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = 512\n",
    "config[\"model\"][\"encoder\"][\"hidden_size\"] = 1024\n",
    "config[\"model\"][\"encoder\"][\"input_size\"] = 512\n",
    "expnum = 1\n",
    "\n",
    "# Unidirectional (1-2-3 layer) Encoder\n",
    "for num_layers in [1,2,3]:\n",
    "    condition= \"{}uni-{}-layer\".format(expnum, num_layers)\n",
    "    config[\"model\"][\"encoder\"][\"num_layers\"] = num_layers\n",
    "    fn = base_filename.format(condition)\n",
    "    exp_list.append(fn)\n",
    "    save_config(fn, config)\n",
    "    expnum+=1\n",
    "    \n",
    "#Bidirectional (1-2-3 layer)\n",
    "for num_layers in [1,2,3]:\n",
    "    condition= \"{}bi-{}-layer\".format(expnum,num_layers)\n",
    "    config[\"model\"][\"encoder\"][\"num_layers\"] = num_layers\n",
    "    config[\"model\"][\"encoder\"][\"bidirectional\"] = True\n",
    "    fn = base_filename.format(condition)\n",
    "    exp_list.append(fn)\n",
    "    save_config(fn, config)\n",
    "    expnum += 1\n",
    "\n",
    "script_fn = os.path.abspath(\"../pbs_scripts/run_all-2.1.sh\")\n",
    "\n",
    "build_bash(script_fn, exp_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For experiment 3.1 (varying encoder depth and directionality on smaller dimensions)\n",
    "\n",
    "config = load_template(template_fn)\n",
    "exp_list = []\n",
    "base_filename = os.path.abspath(\"../experiments/exp-2.1_{}.json\") \n",
    "# common settings\n",
    "\n",
    "encoder_dimensions = [(32,32),(32,64),(64,64),(64,128),(128,128)]\n",
    "\n",
    "for embedding_dim, hidden_dim in encoder_dimensions:\n",
    "    config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = embedding_dim\n",
    "    config[\"model\"][\"encoder\"][\"hidden_size\"] = hidden_dim\n",
    "    config[\"model\"][\"encoder\"][\"input_size\"] = embedding_dim\n",
    "\n",
    "    # Unidirectional (1-2-3 layer) Encoder\n",
    "    for num_layers in [1,2,3]:\n",
    "        condition= \"uni-{}-layer{}-{}\".format(num_layers,embedding_dim,hidden_dim)\n",
    "        config[\"model\"][\"encoder\"][\"num_layers\"] = num_layers\n",
    "        fn = base_filename.format(condition)\n",
    "        exp_list.append(fn)\n",
    "        save_config(fn, config)\n",
    "\n",
    "    #Bidirectional (1-2-3 layer)\n",
    "    for num_layers in [1,2,3]:\n",
    "        condition= \"bi-{}-layer{}-{}\".format(num_layers,embedding_dim,hidden_dim)\n",
    "        config[\"model\"][\"encoder\"][\"num_layers\"] = num_layers\n",
    "        config[\"model\"][\"encoder\"][\"bidirectional\"] = True\n",
    "        fn = base_filename.format(condition)\n",
    "        exp_list.append(fn)\n",
    "        save_config(fn, config)\n",
    "        \n",
    "    script_fn = os.path.abspath(\"../pbs_scripts/run_all-3.sh\")\n",
    "\n",
    "build_bash(script_fn, exp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ca0dae6756ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "# Make the Bash file\n",
    "bash_fn = os.path.abspath(\"../pbs_scripts/run_all-1.1.sh\")\n",
    "header = \"\"\"#Run the experiments\n",
    "set -x\n",
    "source activate allennlp\n",
    "\"\"\"\n",
    "single_exp = \"\"\"allennlp train {conf} -s {serial} --include-package allen_linguo\n",
    "\"\"\"\n",
    "serial_base = os.path.abspath(\"../results\")\n",
    "with open(bash_fn, \"w\") as bash_file:\n",
    "    bash_file.write(header)\n",
    "    for conf_path in exp_files:\n",
    "        exp_name = os.path.basename(conf_path).replace(\".json\",\"\")\n",
    "        serial = os.path.join(serial_base,exp_name)\n",
    "        bash_file.write(single_exp.format(conf=conf_path, serial=serial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For experiment 1.3 Embedding sizes\n",
    "config = load_template(template_fn)\n",
    "\n",
    "dim = 16\n",
    "base_fn = os.path.abspath(\"../experiments/\")\n",
    "\n",
    "base_fn = os.path.join(base_fn, \"exp-1.3_{}-512.json\")\n",
    "exp_files = []\n",
    "#Hidden size is kept at 512\n",
    "config[\"model\"][\"encoder\"][\"hidden_size\"] = 512\n",
    "while dim < 1024:\n",
    "    config[\"model\"][\"word_embeddings\"][\"token_embedders\"][\"tokens\"][\"embedding_dim\"] = dim\n",
    "    config[\"model\"][\"encoder\"][\"input_size\"] = dim    \n",
    "    filename = base_fn.format(dim)\n",
    "    exp_files.append(filename)\n",
    "    save_config(filename, config)\n",
    "    dim = dim*2\n",
    "\n",
    "bash_fn = os.path.abspath(\"../pbs_scripts/run_all-1.3.sh\")\n",
    "\n",
    "build_bash(bash_fn, exp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pablo hola'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{} hola'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (allennlp)",
   "language": "python",
   "name": "allenlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
