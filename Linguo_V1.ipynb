{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a first attempt at a rudimentary sentence grammaticality classifier \n",
    "# It is meant as a feasibility test for the larger experiment and as a learning\n",
    "# Experience for future endeavours\n",
    "\n",
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "#Manual seed for consistency\n",
    "random.seed(42)\n",
    "\n",
    "#More real version that reads in a corpus file\n",
    "ngram_order = 2\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_filename = \"euro.toy\"\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    tokenizer = MosesTokenizer()\n",
    "    parsed_realtext= [tokenizer.tokenize(sentence) for sentence in real_train]\n",
    "    return real_text\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<5:\n",
    "        length = 5\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        linguo.hstate = linguo.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:915.3221575738862\n",
      "1:698.9492537204642\n",
      "2:554.0780851221061\n",
      "3:474.1894234545616\n",
      "4:361.24798116292686\n"
     ]
    }
   ],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "voc_size = len(word_to_ix)\n",
    "hidden_dim = 32\n",
    "epochs = 5\n",
    "linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "optimizer = optim.SGD(linguo.parameters(),lr=0.1)\n",
    "loss_function = nn.NLLLoss()\n",
    "learning_rate=0.1\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(labeled_sentences_train)\n",
    "    for data, label in labeled_sentences_train:\n",
    "        # Restart gradient\n",
    "        linguo.zero_grad()\n",
    "        \n",
    "        \n",
    "        # Run model\n",
    "        in_sentence = prepare_input(word_to_ix,data)\n",
    "        target = autograd.Variable(torch.LongTensor([label]))\n",
    "        prediction = linguo(in_sentence)\n",
    "        #Calculate loss and backpropagate\n",
    "        \n",
    "        #Squared Loss\n",
    "        #loss = torch.pow(target-prediction.view(1),2)\n",
    "        loss = loss_function(prediction,target) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #for parameter in linguo.parameters():\n",
    "        #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "        epoch_loss += loss.data[0]\n",
    "    print(\"{}:{}\".format(i,epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: euro.mini\n",
      "Ngram-Order: 6 \n",
      "Embedding dimension: 32\n",
      "LSTM dimension: 32\n",
      "Hidden Dimension: 32\n",
      "Number of Epochs: 5\n",
      "Final loss:361.24798116292686\n",
      "Accuracy: 0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "# Testing, testing\n",
    "\n",
    "correct = 0\n",
    "salads =[]\n",
    "for testcase in labeled_sentences_test:\n",
    "    prepared_inputs = prepare_input(word_to_ix, testcase[0] )\n",
    "    prediction_vec = linguo(prepared_inputs).view(2)\n",
    "    if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    if prediction == testcase[1]:\n",
    "        correct += 1\n",
    "\n",
    "#Summary:        \n",
    "outtable =\"\"\"Corpus: {corpus}\n",
    "Ngram-Order: {n} \n",
    "Embedding dimension: {embed}\n",
    "LSTM dimension: {lstm}\n",
    "Hidden Dimension: {hidden}\n",
    "Number of Epochs: {epoch}\n",
    "Final loss:{loss}\"\"\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs,\n",
    "                            loss= epoch_loss,\n",
    "                            n=ngram_order)\n",
    "print (outtable)\n",
    "print(\"Accuracy: {}\".format(correct/len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 18582 sentences, 1418 were dumped, among which 853 interogatives or exclamatives\n",
      "Your corpus has 18582 grammatical sentences\n",
      "Word salad data has been generated for order 1\n",
      "\t18582 word salads generated in 0.557 seconds\n",
      "Word salad data has been generated for order 2\n",
      "\t18582 word salads generated in 9.204 seconds\n",
      "Word salad data has been generated for order 3\n",
      "\t18582 word salads generated in 9.045 seconds\n",
      "Word salad data has been generated for order 4\n",
      "\t18582 word salads generated in 9.100 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-42a2759f2976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                       for _ in range(nsal)]\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mn_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ngram_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepro_gram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         word_salads = [generateWSNgram(n_freqs,\n\u001b[1;32m     41\u001b[0m                                      \u001b[0mavg_sent_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-b1a9491cbd06>\u001b[0m in \u001b[0;36mextract_ngram_freq\u001b[0;34m(corpus, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mini\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mini\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mn_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_frequencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell runs the full experiment with cross validation\n",
    "\n",
    "# Modify parameters here\n",
    "corpus_name = \"euro.toy\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 10\n",
    "train_proportion = 0.8\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in parsed_real_train]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(1,max_ngram+1):\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in range(nsal)]\n",
    "    message = \"Word salad data has been generated for order {}\".format(n)\n",
    "    print(message)\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,\n",
    "                                                                      te)\n",
    "    print(message)\n",
    "    message= \"Starting experiment on {}-grams ...\".format(n)\n",
    "    # Iterate over the number of folds\n",
    "    data = prepro_gram + word_salads\n",
    "    for fold in range(folds):\n",
    "        random.shiudata\n",
    "        \n",
    "        # Shuffle and split the data\n",
    "        # Train the Model\n",
    "        # Test the Model\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: euro.mini\n",
    "Ngram-Order: 4 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:504.7399406191398\n",
    "Accuracy: 0.7973684210526316\n",
    "\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 10\n",
    "Final loss:232.77821091443258\n",
    "Accuracy: 0.6868421052631579\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:522.7681364201596\n",
    "Accuracy: 0.7236842105263158\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 2 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:363.4900658682709\n",
    "Accuracy: 0.8184210526315789\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 6 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:361.24798116292686\n",
    "Accuracy: 0.8552631578947368\n",
    "\n",
    "\n",
    "\n",
    "Type           | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy | \n",
    ":--------------|:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 51     | 0.97    |\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |\n",
    "Gram Vs Unigram|euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |\n",
    "Gram Vs Bigram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 718    | 0.81    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 840.8  | 0.67    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 673    | 0.75    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 490.9  | 0.75    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "salads =[]\n",
    "for testcase in labeled_sentences_test:\n",
    "    prepared_inputs = prepare_input(word_to_ix, testcase[0] )\n",
    "    prediction_vec = linguo(prepared_inputs).view(2)\n",
    "    if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    if prediction == testcase[1]:\n",
    "        correct += 1\n",
    "\n",
    "#Summary:        \n",
    "outtable =\"\"\"Corpus: {corpus}\n",
    "Embedding dimension: {embed}\n",
    "LSTM dimension: {lstm}\n",
    "Hidden Dimension: {hidden}\n",
    "Number of Epochs: {epoch}\n",
    "Final loss:{loss}\"\"\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs,\n",
    "                            loss= epoch_loss)\n",
    "print (outtable)\n",
    "print(\"Accuracy: {}\".format(correct/len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reminder for the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "modelfilename= \"Models/{corpus}.{embed}emb.{lstm}lstm.{hidden}hid.{epoch}ep.model\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs)\n",
    "torch.save(linguo.state_dict(), modelfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 3, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "dat = [1,2,3,4,5]\n",
    "random.shuffle(dat)\n",
    "print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examplefile = open(\"example\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "fresh_instances = labeled_sentences_train[:]\n",
    "# Get batches ready\n",
    "num_batches = math.floor(len(fresh_instances)/batch_size)\n",
    "batches = []\n",
    "for i in range(num_batches):\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for j in range(batch_size):\n",
    "        instance = fresh_instances.pop()\n",
    "        data_tensor = prepare_input(word_to_ix, instance[0])\n",
    "        batch_data.append(data_tensor)\n",
    "        print(data_tensor.size())\n",
    "        batch_labels.append(int(instance[1]))\n",
    "    batch_var = torch.stack(batch_data)\n",
    "    batch_labels_var = autograd.Variable(torch.IntTensor(batch_labels))\n",
    "    batches.append(batch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "corp = [[\"Yo\",\"como\",\"mucha\",\"pizza\"], [\"Yo\",\"tengo\",\"mucha\",\"hambre\"]]\n",
    "\n",
    "corp[0].insert(0,\"#\")\n",
    "corp[0].insert(len(corp[0]),\"#\")\n",
    "\n",
    "fdist = extract_ngram_freq(corp,2)\n",
    "generateWSNgram(fdist,3,0.1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstWord = choice(fwvocab,1,fwfreqs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
