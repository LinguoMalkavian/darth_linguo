{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a first attempt at a rudimentary sentence grammaticality classifier \n",
    "# It is meant as a feasibility test for the larger experiment and as a learning\n",
    "# Experience for future endeavours\n",
    "\n",
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    tokenizer = MosesTokenizer()\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in real_text]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<5:\n",
    "        length = 5\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        self.hstate = self.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "def train_model(train_data,\n",
    "                embed_dim,\n",
    "                lstm_dim,\n",
    "                hidden_dim,\n",
    "                word_to_ix,\n",
    "                epochs,\n",
    "                learning_rate):\n",
    "    voc_size = len(word_to_ix)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word_to_ix,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"\\t Epoch{}:{}\".format(i,epoch_loss))\n",
    "    return linguo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, testing\n",
    "def test_model(test_data,model,word2id):\n",
    "    correct = 0.0\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for testcase in test_data:\n",
    "        target = testcase[1]\n",
    "        prepared_inputs = prepare_input(word2id, testcase[0] )\n",
    "        prediction_vec = model(prepared_inputs).view(2)\n",
    "        if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if prediction == testcase[1]:\n",
    "            correct += 1\n",
    "            if target == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                fn +=1\n",
    "            else:\n",
    "                fp +=1\n",
    "                \n",
    "    # Compile results\n",
    "    accuracy = correct/len(test_data)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    fmeasure = 2*tp / (2*tp+fp+fn) \n",
    "    results = {\"accuracy\":accuracy,\n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "               \"fmeasure\":fmeasure,\n",
    "              \"tp\":tp,\n",
    "              \"tn\":tn,\n",
    "              \"fp\":fp,\n",
    "              \"fn\":fn}\n",
    "    return results\n",
    "\n",
    "def average_results(result_list):\n",
    "    total = len(result_list)\n",
    "    averaged =defaultdict(float)\n",
    "    for report in result_list:\n",
    "        for item in report:\n",
    "            averaged[item] += report[item]\n",
    "    for item in averaged:\n",
    "        averaged[item] = averaged[item]/total\n",
    "    return averaged\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 85 sentences, 15 were dumped, among which 10 interogatives or exclamatives\n",
      "Your corpus has 85 grammatical sentences\n",
      "Word salad data has been generated for order 1\n",
      "\t85 word salads generated in 0.005 seconds\n",
      "Starting training on fold 1 for 1-grams...\n",
      "\t Epoch0:80.69978695362806\n",
      "\t Epoch1:29.34754234342836\n",
      "\t Epoch2:2.802400938468054\n",
      "Training finished in 9.3773 seconds, starting testing...\n",
      "Testing finished in 0.2557189464569092 seconds\n",
      "Accuracy is 0.9411764705882353\n",
      "Starting training on fold 2 for 1-grams...\n",
      "\t Epoch0:56.69546570442617\n",
      "\t Epoch1:3.4281011104467325\n",
      "\t Epoch2:0.43643392651574686\n",
      "Training finished in 9.2925 seconds, starting testing...\n",
      "Testing finished in 0.23580193519592285 seconds\n",
      "Accuracy is 1.0\n",
      "Starting training on fold 3 for 1-grams...\n",
      "\t Epoch0:67.73590994300321\n",
      "\t Epoch1:18.96804113825783\n",
      "\t Epoch2:2.9427458466379903\n",
      "Training finished in 9.6345 seconds, starting testing...\n",
      "Testing finished in 0.2566680908203125 seconds\n",
      "Accuracy is 1.0\n",
      "Starting training on fold 4 for 1-grams...\n",
      "\t Epoch0:78.82555894553661\n",
      "\t Epoch1:34.28452917467803\n",
      "\t Epoch2:24.758055482030613\n",
      "Training finished in 9.2505 seconds, starting testing...\n",
      "Testing finished in 0.2549419403076172 seconds\n",
      "Accuracy is 1.0\n",
      "Starting training on fold 5 for 1-grams...\n",
      "\t Epoch0:79.69044141843915\n",
      "\t Epoch1:39.437572912313044\n",
      "\t Epoch2:11.735330581665039\n",
      "Training finished in 9.1746 seconds, starting testing...\n",
      "Testing finished in 0.24794387817382812 seconds\n",
      "Accuracy is 0.9117647058823529\n",
      "Starting training on fold 6 for 1-grams...\n",
      "\t Epoch0:75.50197668932378\n",
      "\t Epoch1:28.586139281047508\n",
      "\t Epoch2:5.397215890057851\n",
      "Training finished in 9.1746 seconds, starting testing...\n",
      "Testing finished in 0.2880232334136963 seconds\n",
      "Accuracy is 1.0\n",
      "Starting training on fold 7 for 1-grams...\n",
      "\t Epoch0:61.273370140232146\n",
      "\t Epoch1:14.011488030664623\n",
      "\t Epoch2:3.151322536636144\n",
      "Training finished in 9.4637 seconds, starting testing...\n",
      "Testing finished in 0.21521520614624023 seconds\n",
      "Accuracy is 1.0\n",
      "Starting training on fold 8 for 1-grams...\n",
      "\t Epoch0:84.03742940723896\n",
      "\t Epoch1:43.70491670817137\n",
      "\t Epoch2:33.75261913985014\n",
      "Training finished in 8.9482 seconds, starting testing...\n",
      "Testing finished in 0.300731897354126 seconds\n",
      "Accuracy is 0.9705882352941176\n",
      "Starting training on fold 9 for 1-grams...\n",
      "\t Epoch0:61.977723011747\n",
      "\t Epoch1:20.776477341074497\n",
      "\t Epoch2:6.2679743961198255\n",
      "Training finished in 8.8316 seconds, starting testing...\n",
      "Testing finished in 0.29228901863098145 seconds\n",
      "Accuracy is 0.9705882352941176\n",
      "Starting training on fold 10 for 1-grams...\n",
      "\t Epoch0:67.17954380530864\n",
      "\t Epoch1:34.960143654607236\n",
      "\t Epoch2:15.598587428801693\n",
      "Training finished in 9.2348 seconds, starting testing...\n",
      "Testing finished in 0.28510499000549316 seconds\n",
      "Accuracy is 0.9117647058823529\n",
      "Results are in for 1-grams\n",
      "\tFinished 10 folds in 95.0279 s\n",
      "\tAverage accuracy is:0.9705882352941178\n",
      "\tAverage F measure is:0.9700270562770562\n",
      "Word salad data has been generated for order 2\n",
      "\t85 word salads generated in 0.073 seconds\n",
      "Starting training on fold 1 for 2-grams...\n",
      "\t Epoch0:93.88039928674698\n",
      "\t Epoch1:87.27766449004412\n",
      "\t Epoch2:81.22097324579954\n",
      "Training finished in 10.6492 seconds, starting testing...\n",
      "Testing finished in 0.21712183952331543 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Starting training on fold 2 for 2-grams...\n",
      "\t Epoch0:96.95044639706612\n",
      "\t Epoch1:89.20160542428493\n",
      "\t Epoch2:83.57931852340698\n",
      "Training finished in 10.1691 seconds, starting testing...\n",
      "Testing finished in 0.2512691020965576 seconds\n",
      "Accuracy is 0.5\n",
      "Starting training on fold 3 for 2-grams...\n",
      "\t Epoch0:94.54879543185234\n",
      "\t Epoch1:90.19532936811447\n",
      "\t Epoch2:82.40034318715334\n",
      "Training finished in 10.3816 seconds, starting testing...\n",
      "Testing finished in 0.27042102813720703 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 4 for 2-grams...\n",
      "\t Epoch0:94.17246916890144\n",
      "\t Epoch1:88.03158991038799\n",
      "\t Epoch2:79.82941555976868\n",
      "Training finished in 10.5156 seconds, starting testing...\n",
      "Testing finished in 0.21187520027160645 seconds\n",
      "Accuracy is 0.7941176470588235\n",
      "Starting training on fold 5 for 2-grams...\n",
      "\t Epoch0:94.85105921328068\n",
      "\t Epoch1:85.0790398772806\n",
      "\t Epoch2:74.80322235450149\n",
      "Training finished in 10.8627 seconds, starting testing...\n",
      "Testing finished in 0.20859479904174805 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Starting training on fold 6 for 2-grams...\n",
      "\t Epoch0:91.92521339654922\n",
      "\t Epoch1:88.08409915864468\n",
      "\t Epoch2:83.78905396163464\n",
      "Training finished in 10.1704 seconds, starting testing...\n",
      "Testing finished in 0.24793291091918945 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Starting training on fold 7 for 2-grams...\n",
      "\t Epoch0:92.93893972039223\n",
      "\t Epoch1:89.8266698718071\n",
      "\t Epoch2:77.20237344875932\n",
      "Training finished in 10.1945 seconds, starting testing...\n",
      "Testing finished in 0.29216527938842773 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 8 for 2-grams...\n",
      "\t Epoch0:97.4638400375843\n",
      "\t Epoch1:91.4843158274889\n",
      "\t Epoch2:87.21873617917299\n",
      "Training finished in 10.3666 seconds, starting testing...\n",
      "Testing finished in 0.24592375755310059 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Starting training on fold 9 for 2-grams...\n",
      "\t Epoch0:95.64731851220131\n",
      "\t Epoch1:84.28336074948311\n",
      "\t Epoch2:83.92365463078022\n",
      "Training finished in 10.2490 seconds, starting testing...\n",
      "Testing finished in 0.2357177734375 seconds\n",
      "Accuracy is 0.6470588235294118\n",
      "Starting training on fold 10 for 2-grams...\n",
      "\t Epoch0:91.26896452903748\n",
      "\t Epoch1:83.06558533012867\n",
      "\t Epoch2:72.81546636670828\n",
      "Training finished in 10.1881 seconds, starting testing...\n",
      "Testing finished in 0.2902510166168213 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Results are in for 2-grams\n",
      "\tFinished 10 folds in 106.2985 s\n",
      "\tAverage accuracy is:0.6176470588235294\n",
      "\tAverage F measure is:0.6430227388428895\n",
      "Word salad data has been generated for order 3\n",
      "\t85 word salads generated in 0.134 seconds\n",
      "Starting training on fold 1 for 3-grams...\n",
      "\t Epoch0:95.50171345472336\n",
      "\t Epoch1:84.1505899950862\n",
      "\t Epoch2:75.81495126336813\n",
      "Training finished in 10.9546 seconds, starting testing...\n",
      "Testing finished in 0.25412702560424805 seconds\n",
      "Accuracy is 0.7352941176470589\n",
      "Starting training on fold 2 for 3-grams...\n",
      "\t Epoch0:90.44302140176296\n",
      "\t Epoch1:76.60675182566047\n",
      "\t Epoch2:65.78201411478221\n",
      "Training finished in 11.6245 seconds, starting testing...\n",
      "Testing finished in 0.24201512336730957 seconds\n",
      "Accuracy is 0.5294117647058824\n",
      "Starting training on fold 3 for 3-grams...\n",
      "\t Epoch0:94.58515858650208\n",
      "\t Epoch1:87.22098177671432\n",
      "\t Epoch2:76.39779557473958\n",
      "Training finished in 11.3121 seconds, starting testing...\n",
      "Testing finished in 0.23250198364257812 seconds\n",
      "Accuracy is 0.8823529411764706\n",
      "Starting training on fold 4 for 3-grams...\n",
      "\t Epoch0:93.9515842795372\n",
      "\t Epoch1:81.60166027396917\n",
      "\t Epoch2:71.32290353253484\n",
      "Training finished in 11.2353 seconds, starting testing...\n",
      "Testing finished in 0.24266529083251953 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 5 for 3-grams...\n",
      "\t Epoch0:94.11696285009384\n",
      "\t Epoch1:89.19094035029411\n",
      "\t Epoch2:79.4508993551135\n",
      "Training finished in 10.6287 seconds, starting testing...\n",
      "Testing finished in 0.2861061096191406 seconds\n",
      "Accuracy is 0.7647058823529411\n",
      "Starting training on fold 6 for 3-grams...\n",
      "\t Epoch0:94.12231804430485\n",
      "\t Epoch1:85.88041180372238\n",
      "\t Epoch2:75.19670838862658\n",
      "Training finished in 11.7243 seconds, starting testing...\n",
      "Testing finished in 0.28306007385253906 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 7 for 3-grams...\n",
      "\t Epoch0:94.09187261760235\n",
      "\t Epoch1:86.66174057871103\n",
      "\t Epoch2:79.52302638068795\n",
      "Training finished in 11.3322 seconds, starting testing...\n",
      "Testing finished in 0.2804598808288574 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 8 for 3-grams...\n",
      "\t Epoch0:92.33527168631554\n",
      "\t Epoch1:73.28490940481424\n",
      "\t Epoch2:60.140614461153746\n",
      "Training finished in 11.0638 seconds, starting testing...\n",
      "Testing finished in 0.31931376457214355 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 9 for 3-grams...\n",
      "\t Epoch0:90.67194198817015\n",
      "\t Epoch1:78.13592487573624\n",
      "\t Epoch2:63.159913720563054\n",
      "Training finished in 11.6208 seconds, starting testing...\n",
      "Testing finished in 0.2356562614440918 seconds\n",
      "Accuracy is 0.7058823529411765\n",
      "Starting training on fold 10 for 3-grams...\n",
      "\t Epoch0:94.21523210406303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch1:86.21056123822927\n",
      "\t Epoch2:78.95104770362377\n",
      "Training finished in 11.3454 seconds, starting testing...\n",
      "Testing finished in 0.23978185653686523 seconds\n",
      "Accuracy is 0.6470588235294118\n",
      "Results are in for 3-grams\n",
      "\tFinished 10 folds in 115.6036 s\n",
      "\tAverage accuracy is:0.6882352941176471\n",
      "\tAverage F measure is:0.679341661040011\n",
      "Word salad data has been generated for order 4\n",
      "\t85 word salads generated in 0.102 seconds\n",
      "Starting training on fold 1 for 4-grams...\n",
      "\t Epoch0:94.19298441708088\n",
      "\t Epoch1:86.73092463612556\n",
      "\t Epoch2:77.06185767054558\n",
      "Training finished in 11.8251 seconds, starting testing...\n",
      "Testing finished in 0.3143627643585205 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 2 for 4-grams...\n",
      "\t Epoch0:91.26314596831799\n",
      "\t Epoch1:82.78708214312792\n",
      "\t Epoch2:83.56644394993782\n",
      "Training finished in 12.7646 seconds, starting testing...\n",
      "Testing finished in 0.3484013080596924 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 3 for 4-grams...\n",
      "\t Epoch0:95.320416867733\n",
      "\t Epoch1:91.36086592078209\n",
      "\t Epoch2:82.91580761224031\n",
      "Training finished in 11.3605 seconds, starting testing...\n",
      "Testing finished in 0.33251500129699707 seconds\n",
      "Accuracy is 0.7647058823529411\n",
      "Starting training on fold 4 for 4-grams...\n",
      "\t Epoch0:95.82257559895515\n",
      "\t Epoch1:90.6513155400753\n",
      "\t Epoch2:83.00736932829022\n",
      "Training finished in 13.1400 seconds, starting testing...\n",
      "Testing finished in 0.28254175186157227 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Starting training on fold 5 for 4-grams...\n",
      "\t Epoch0:95.11758714914322\n",
      "\t Epoch1:84.34229802340269\n",
      "\t Epoch2:72.56562619656324\n",
      "Training finished in 13.9291 seconds, starting testing...\n",
      "Testing finished in 0.26311826705932617 seconds\n",
      "Accuracy is 0.6470588235294118\n",
      "Starting training on fold 6 for 4-grams...\n",
      "\t Epoch0:88.76384349167347\n",
      "\t Epoch1:83.70823141932487\n",
      "\t Epoch2:72.44739934056997\n",
      "Training finished in 12.7048 seconds, starting testing...\n",
      "Testing finished in 0.2953300476074219 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Starting training on fold 7 for 4-grams...\n",
      "\t Epoch0:95.380944699049\n",
      "\t Epoch1:89.93770915269852\n",
      "\t Epoch2:83.61875561624765\n",
      "Training finished in 11.2432 seconds, starting testing...\n",
      "Testing finished in 0.3057839870452881 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 8 for 4-grams...\n",
      "\t Epoch0:94.7633948624134\n",
      "\t Epoch1:86.36559794843197\n",
      "\t Epoch2:79.66427154839039\n",
      "Training finished in 11.3074 seconds, starting testing...\n",
      "Testing finished in 0.3152322769165039 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 9 for 4-grams...\n",
      "\t Epoch0:95.7865774333477\n",
      "\t Epoch1:91.90554401278496\n",
      "\t Epoch2:89.59530441462994\n",
      "Training finished in 11.3924 seconds, starting testing...\n",
      "Testing finished in 0.2914292812347412 seconds\n",
      "Accuracy is 0.7058823529411765\n",
      "Starting training on fold 10 for 4-grams...\n",
      "\t Epoch0:96.39007046818733\n",
      "\t Epoch1:84.87011646479368\n",
      "\t Epoch2:82.38723580911756\n",
      "Training finished in 11.7623 seconds, starting testing...\n",
      "Testing finished in 0.286268949508667 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Results are in for 4-grams\n",
      "\tFinished 10 folds in 124.5786 s\n",
      "\tAverage accuracy is:0.6470588235294118\n",
      "\tAverage F measure is:0.6848657536265664\n",
      "Word salad data has been generated for order 5\n",
      "\t85 word salads generated in 0.094 seconds\n",
      "Starting training on fold 1 for 5-grams...\n",
      "\t Epoch0:88.25216779857874\n",
      "\t Epoch1:69.54937181994319\n",
      "\t Epoch2:67.99917912855744\n",
      "Training finished in 12.5072 seconds, starting testing...\n",
      "Testing finished in 0.3198399543762207 seconds\n",
      "Accuracy is 0.7058823529411765\n",
      "Starting training on fold 2 for 5-grams...\n",
      "\t Epoch0:91.22939217090607\n",
      "\t Epoch1:87.65927385538816\n",
      "\t Epoch2:78.58881461247802\n",
      "Training finished in 12.8979 seconds, starting testing...\n",
      "Testing finished in 0.28899502754211426 seconds\n",
      "Accuracy is 0.7352941176470589\n",
      "Starting training on fold 3 for 5-grams...\n",
      "\t Epoch0:94.76371428370476\n",
      "\t Epoch1:85.30111072957516\n",
      "\t Epoch2:76.07982534915209\n",
      "Training finished in 11.8581 seconds, starting testing...\n",
      "Testing finished in 0.3635129928588867 seconds\n",
      "Accuracy is 0.8235294117647058\n",
      "Starting training on fold 4 for 5-grams...\n",
      "\t Epoch0:90.50406318157911\n",
      "\t Epoch1:75.80351241678\n",
      "\t Epoch2:67.51996514946222\n",
      "Training finished in 12.8926 seconds, starting testing...\n",
      "Testing finished in 0.3106081485748291 seconds\n",
      "Accuracy is 0.5294117647058824\n",
      "Starting training on fold 5 for 5-grams...\n",
      "\t Epoch0:94.4141776561737\n",
      "\t Epoch1:87.88122847676277\n",
      "\t Epoch2:73.90017681568861\n",
      "Training finished in 12.6379 seconds, starting testing...\n",
      "Testing finished in 0.3357429504394531 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 6 for 5-grams...\n",
      "\t Epoch0:94.6207825243473\n",
      "\t Epoch1:84.93230461329222\n",
      "\t Epoch2:80.51391224563122\n",
      "Training finished in 12.7827 seconds, starting testing...\n",
      "Testing finished in 0.3172018527984619 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 7 for 5-grams...\n",
      "\t Epoch0:93.0333124101162\n",
      "\t Epoch1:85.48603238910437\n",
      "\t Epoch2:75.56911580264568\n",
      "Training finished in 13.0239 seconds, starting testing...\n",
      "Testing finished in 0.27556419372558594 seconds\n",
      "Accuracy is 0.8235294117647058\n",
      "Starting training on fold 8 for 5-grams...\n",
      "\t Epoch0:92.17583568394184\n",
      "\t Epoch1:74.67925074324012\n",
      "\t Epoch2:69.47642355971038\n",
      "Training finished in 12.8260 seconds, starting testing...\n",
      "Testing finished in 0.3458068370819092 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 9 for 5-grams...\n",
      "\t Epoch0:91.60010528564453\n",
      "\t Epoch1:78.26932276785374\n",
      "\t Epoch2:70.25915695726871\n",
      "Training finished in 13.2776 seconds, starting testing...\n",
      "Testing finished in 0.2735898494720459 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 10 for 5-grams...\n",
      "\t Epoch0:92.0929538756609\n",
      "\t Epoch1:71.39867322146893\n",
      "\t Epoch2:58.39050061814487\n",
      "Training finished in 14.1892 seconds, starting testing...\n",
      "Testing finished in 0.29850125312805176 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Results are in for 5-grams\n",
      "\tFinished 10 folds in 132.1297 s\n",
      "\tAverage accuracy is:0.676470588235294\n",
      "\tAverage F measure is:0.6658536028155522\n",
      "Word salad data has been generated for order 6\n",
      "\t85 word salads generated in 0.127 seconds\n",
      "Starting training on fold 1 for 6-grams...\n",
      "\t Epoch0:90.21873989701271\n",
      "\t Epoch1:75.91642335057259\n",
      "\t Epoch2:68.90733356028795\n",
      "Training finished in 15.9364 seconds, starting testing...\n",
      "Testing finished in 0.3289930820465088 seconds\n",
      "Accuracy is 0.6470588235294118\n",
      "Starting training on fold 2 for 6-grams...\n",
      "\t Epoch0:93.7577731013298\n",
      "\t Epoch1:81.99655194953084\n",
      "\t Epoch2:66.81439473852515\n",
      "Training finished in 15.0799 seconds, starting testing...\n",
      "Testing finished in 0.3669121265411377 seconds\n",
      "Accuracy is 0.5882352941176471\n",
      "Starting training on fold 3 for 6-grams...\n",
      "\t Epoch0:92.11263385415077\n",
      "\t Epoch1:70.1905565969646\n",
      "\t Epoch2:60.47452046349645\n",
      "Training finished in 14.8506 seconds, starting testing...\n",
      "Testing finished in 0.349409818649292 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Starting training on fold 4 for 6-grams...\n",
      "\t Epoch0:89.90913590788841\n",
      "\t Epoch1:73.32133288681507\n",
      "\t Epoch2:60.946900457143784\n",
      "Training finished in 13.9383 seconds, starting testing...\n",
      "Testing finished in 0.35426902770996094 seconds\n",
      "Accuracy is 0.7058823529411765\n",
      "Starting training on fold 5 for 6-grams...\n",
      "\t Epoch0:92.73714517056942\n",
      "\t Epoch1:80.43247452378273\n",
      "\t Epoch2:70.04889487475157\n",
      "Training finished in 14.7794 seconds, starting testing...\n",
      "Testing finished in 0.3768949508666992 seconds\n",
      "Accuracy is 0.7941176470588235\n",
      "Starting training on fold 6 for 6-grams...\n",
      "\t Epoch0:91.709383495152\n",
      "\t Epoch1:74.49810713529587\n",
      "\t Epoch2:65.4660716317594\n",
      "Training finished in 15.7660 seconds, starting testing...\n",
      "Testing finished in 0.32721996307373047 seconds\n",
      "Accuracy is 0.6176470588235294\n",
      "Starting training on fold 7 for 6-grams...\n",
      "\t Epoch0:93.12304659187794\n",
      "\t Epoch1:81.47597613930702\n",
      "\t Epoch2:70.71251109614968\n",
      "Training finished in 14.6931 seconds, starting testing...\n",
      "Testing finished in 0.3612232208251953 seconds\n",
      "Accuracy is 0.6764705882352942\n",
      "Starting training on fold 8 for 6-grams...\n",
      "\t Epoch0:93.31391380727291\n",
      "\t Epoch1:83.54348792880774\n",
      "\t Epoch2:76.44255194813013\n",
      "Training finished in 14.8265 seconds, starting testing...\n",
      "Testing finished in 0.2885251045227051 seconds\n",
      "Accuracy is 0.7352941176470589\n",
      "Starting training on fold 9 for 6-grams...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch0:96.06741347908974\n",
      "\t Epoch1:83.28024087846279\n",
      "\t Epoch2:75.43623610585928\n",
      "Training finished in 13.9613 seconds, starting testing...\n",
      "Testing finished in 0.3949298858642578 seconds\n",
      "Accuracy is 0.6470588235294118\n",
      "Starting training on fold 10 for 6-grams...\n",
      "\t Epoch0:94.28515034914017\n",
      "\t Epoch1:83.35647625476122\n",
      "\t Epoch2:75.3218812122941\n",
      "Training finished in 13.7476 seconds, starting testing...\n",
      "Testing finished in 0.387692928314209 seconds\n",
      "Accuracy is 0.5588235294117647\n",
      "Results are in for 6-grams\n",
      "\tFinished 10 folds in 151.2590 s\n",
      "\tAverage accuracy is:0.6529411764705882\n",
      "\tAverage F measure is:0.6904554332866494\n"
     ]
    }
   ],
   "source": [
    "# This cell runs the full experiment with cross validation\n",
    "\n",
    "# Modify parameters here\n",
    "corpus_name = \"euro.micro\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 10\n",
    "train_proportion = 0.8\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "hidden_dim = 32\n",
    "epochs = 3\n",
    "learning_rate=0.1\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in prepro_gram]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "full_results = []\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(1,max_ngram+1):\n",
    "    t2 = time()\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in range(nsal)]\n",
    "    \n",
    "    labeled_g = [[sentence,1] for sentence in prepro_gram]\n",
    "    labeled_ws = [[sentence,0] for sentence in word_salads]\n",
    "    \n",
    "    message = \"Word salad data has been generated for order {}\".format(n)\n",
    "    print(message)\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,\n",
    "                                                                      te)\n",
    "    print(message)\n",
    "    message= \"Starting experiment on {}-grams ...\".format(n)\n",
    "    \n",
    "    result_list = []\n",
    "    # Iterate over the number of folds\n",
    "    for fold in range(folds):\n",
    "        t1 = time()\n",
    "        message = \"Starting training on fold {} for {}-grams...\".format(fold+1,n)\n",
    "        print(message)\n",
    "        # Shuffle and split data\n",
    "        random.shuffle(labeled_g)\n",
    "        random.shuffle(labeled_ws)\n",
    "        cutoff = math.floor(train_proportion * len(labeled_g))\n",
    "        train_g, test_g = labeled_g[:cutoff],labeled_g[cutoff:]\n",
    "        train_ws,test_ws = labeled_ws[:cutoff],labeled_ws[cutoff:]\n",
    "        \n",
    "        train_data = train_g + train_ws\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        test_data = test_g + test_ws\n",
    "        random.shuffle(test_data)\n",
    "        \n",
    "        # Train the Model\n",
    "        model = train_model(train_data,\n",
    "                            embed_dim,\n",
    "                            lstm_dim,\n",
    "                            hidden_dim,\n",
    "                            word2id,\n",
    "                            epochs,\n",
    "                            learning_rate)\n",
    "        te = time()-t1\n",
    "        message = \"Training finished in {:.4f} seconds, starting testing...\".format(te)\n",
    "        print(message)\n",
    "        print(\"...\")\n",
    "        t1 = time()\n",
    "        # Test the Model\n",
    "        fold_results = test_model(test_data,model,word2id)\n",
    "        result_list.append(fold_results)\n",
    "        te = time()-t1\n",
    "        message = \"Testing finished in {} seconds\".format(te)\n",
    "        print(message)\n",
    "        message = \"\\Accuracy is {}\".format(fold_results['accuracy'])\n",
    "        print(message)\n",
    "        \n",
    "    order_results = average_results(result_list)\n",
    "    te2 = time()- t2\n",
    "    message=\"Results are in for {}-grams\".format(n)\n",
    "    print(message)\n",
    "    message=\"\\tFinished {} folds in {:.4f} s\".format(folds,te2)\n",
    "    print(message)\n",
    "    message=\"\\tAverage accuracy is:{}\".format(order_results[\"accuracy\"])\n",
    "    print(message)\n",
    "    message=\"\\tAverage F measure is:{}\".format(order_results[\"fmeasure\"])\n",
    "    print(message)\n",
    "    full_results.append(order_results)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'accuracy': 0.7742105263157895,\n",
       "             'fmeasure': 0.7733546517277098,\n",
       "             'fn': 39.0,\n",
       "             'fp': 46.8,\n",
       "             'precision': 0.770421382714067,\n",
       "             'recall': 0.7947368421052632,\n",
       "             'tn': 143.2,\n",
       "             'tp': 151.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_results = average_results(result_list)\n",
    "order_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: euro.mini\n",
    "Ngram-Order: 4 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:504.7399406191398\n",
    "Accuracy: 0.7973684210526316\n",
    "\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 10\n",
    "Final loss:232.77821091443258\n",
    "Accuracy: 0.6868421052631579\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:522.7681364201596\n",
    "Accuracy: 0.7236842105263158\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 2 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:363.4900658682709\n",
    "Accuracy: 0.8184210526315789\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 6 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:361.24798116292686\n",
    "Accuracy: 0.8552631578947368\n",
    "\n",
    "\n",
    "\n",
    "Type           | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy | \n",
    ":--------------|:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 51     | 0.97    |\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |\n",
    "Gram Vs Unigram|euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |\n",
    "Gram Vs Bigram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 718    | 0.81    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 840.8  | 0.67    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 673    | 0.75    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 490.9  | 0.75    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
