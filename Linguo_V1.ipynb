{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a first attempt at a rudimentary sentence grammaticality classifier \n",
    "# It is meant as a feasibility test for the larger experiment and as a learning\n",
    "# Experience for future endeavours\n",
    "\n",
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#spacy model\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "#load Spacy spanish model to handle tokenization of toy data\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 18582 sentences, 1418 were dumped, among which 853 interogatives or exclamatives\n",
      "Done, you now have 29730 train instances and 7434 test instancess:\n"
     ]
    }
   ],
   "source": [
    "# Handles importing data, default version takes mini toy, further will take full sentences.\n",
    "\n",
    "#Manual seed for consistency\n",
    "random.seed(42)\n",
    "#This is the kid version\n",
    "# real_text = [\"Los acontecimientos tienen lugar en una galaxia ficticia de nombre desconocido y en un tiempo no especificado.\", \"Además de la raza humana, son descritos muchos tipos de especies extraterrestres procedentes de los numerosos planetas y satélites que forman dicha galaxia y pertenecen a la alianza de planetas de la República Galáctica.\", \"Otros personajes recurrentes son los robots y los androides, creados generalmente para servir a un propósito, observándose así droides astromecánicos, médicos, de protocolo, de combate, entre otros.\",\n",
    "# \"Los viajes espaciales son comunes y la mayoría de los planetas que aparecen en la saga están afiliados a la República Galáctica, la unión democrática que rige la galaxia y cuyo gobierno, presidido por un Canciller Supremo, está formado por representantes elegidos o designados de toda ella agrupados en el llamado Senado Galáctico, ubicado en el planeta Coruscant.\", \"En oposición a la República se encuentra la Confederación de Sistemas Independientes, siendo el enfrentamiento de ambas uno de los temas más importantes en la trama de las tres primeras películas de Star Wars.\", \"Uno de los elementos principales en la saga es «la Fuerza», un campo de energía metafísico y omnipresente creado por las cosas que existen, que impregna el universo y todo lo que hay en él.\", \"La Orden Jedi es una organización de caballeros unidos por su creencia y percepción de la Fuerza, que luchan por la paz y la justicia en la República Galáctica.\",\"Se entrenan en el uso del sable de luz o espada láser, un arma similar a una espada tradicional salvo por el hecho que su hoja es un haz de energía.\",\n",
    "# \"Los Jedi son capaces de manejar la Fuerza y lograr así habilidades como la telequinesis, la clarividencia, el control mental o una amplificación de los reflejos, la velocidad y otras capacidades físicas.\", \"No obstante y aunque dicho grupo la utiliza con fines positivos, tiene un lado oscuro provocado por la ira, el miedo y el odio.\", \"Este lado es usado por los sith con el fin de exterminar a los jedi y tomar el control de la Galaxia.\"]\n",
    "\n",
    "#More real version that reads in a corpus file\n",
    "ngram_order = 2\n",
    "\n",
    "\n",
    "input_corpus_filename = \"euro.toy\"\n",
    "input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "in_file = open(input_corpus_path, \"r\")\n",
    "real_text = []\n",
    "numlines = 0\n",
    "inter_excl=0\n",
    "for line in in_file.readlines():\n",
    "    #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "    if line.strip() !=\"\":\n",
    "        if line.strip()[-1] == \".\":\n",
    "            real_text.append(line.strip())\n",
    "        elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "            inter_excl +=1\n",
    "    numlines+=1\n",
    "\n",
    "print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                            len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "proportion_train = 0.8\n",
    "cutoff = math.floor(len(real_text)*proportion_train)\n",
    "random.shuffle(real_text)\n",
    "\n",
    "real_train , real_test = real_text[:cutoff], real_text[cutoff:]\n",
    "\n",
    "# Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "parsed_real_train = [nlp(sentence) for sentence in real_train]\n",
    "parsed_real_test = [nlp(sentence) for sentence in real_test]\n",
    "\n",
    "#Extract the statististical info needed to generate unigram word salad\n",
    "#Calculate average sentence length\n",
    "lengths= [len(sent) for sent in parsed_real_train]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "counts = defaultdict(int)\n",
    "total = 0.0\n",
    "for sentence in parsed_real_train:\n",
    "    for token in sentence:\n",
    "        if token.text != \".\":\n",
    "            counts[token.text.lower()] +=1\n",
    "            total += 1\n",
    "\n",
    "# TODO: implement a version where low frequency words are replaced by their tag\n",
    "#Switch happaxes for the UNK token\n",
    "hapaxes = []\n",
    "counts[\"#unk\"]=0\n",
    "for key in counts:\n",
    "    if counts[key] == 1:\n",
    "        counts[\"#unk\"] += 1\n",
    "        hapaxes.append(key)\n",
    "\n",
    "for hapax in hapaxes:\n",
    "    counts.pop(hapax)\n",
    "\n",
    "vocabulary = []\n",
    "probdist = []\n",
    "for key in counts:\n",
    "    vocabulary.append(key)\n",
    "    probdist.append(counts[key])\n",
    "\n",
    "\n",
    "# In tokenized we only retain the sentences as an array of words and we implement the replacements.\n",
    "# We pre tokenize test data taking only the hapaxes from train out \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "\n",
    "def token_replacement(parsed_sentences, hapaxes):\n",
    "    # Takes a list of sentences that have gone through the spacy pipeline\n",
    "    # (sentences have Doc type)\n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    # TODO: implement a version that replaces words by their tag instead\n",
    "    tokenized = []\n",
    "    for sentence in parsed_sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.text.lower() in hapaxes:\n",
    "                this_sentence.append(\"#UNK\")\n",
    "            else:\n",
    "                this_sentence.append(token.text)\n",
    "        tokenized.append(this_sentence)\n",
    "    return tokenized\n",
    "\n",
    "# Get the sentences represented as lists of words\n",
    "tokenized_real_train = token_replacement(parsed_real_train, hapaxes)\n",
    "tokenized_real_test = token_replacement(parsed_real_test, hapaxes)\n",
    "\n",
    "#Extract n-gram frequencies\n",
    "\n",
    "\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order , unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "n_gram_frequencies = extract_ngram_freq(tokenized_real_train,ngram_order)\n",
    "\n",
    "# get a list of word salads the same length as the real test data    \n",
    "word_salads_train = [generateWSNgram(n_gram_frequencies, \n",
    "                          avg_sent_length,length_sd, ngram_order,counts) for _ in range(len(tokenized_real_train))]\n",
    "word_salads_test = [generateWSNgram(n_gram_frequencies, \n",
    "                          avg_sent_length,length_sd, ngram_order,counts) for _ in range(len(tokenized_real_test))]\n",
    "\n",
    "#Consolidate training data\n",
    "labeled_sentences_train = [[sentence, 1] for sentence in tokenized_real_train]\n",
    "labeled_sentences_train += [[sentence, 0] for sentence in word_salads_train]\n",
    "random.shuffle(labeled_sentences_train)\n",
    "\n",
    "#Consolidate test data\n",
    "labeled_sentences_test = [[sentence, 1] for sentence in tokenized_real_test]\n",
    "labeled_sentences_test += [[sentence, 0] for sentence in word_salads_test]\n",
    "random.shuffle(labeled_sentences_test)\n",
    "\n",
    "#Define the vocabulary and word ids\n",
    "vocabulary.append(\".\")\n",
    "\n",
    "word_to_ix = {}\n",
    "for word in vocabulary:\n",
    "    word_to_ix[word] = len(word_to_ix)\n",
    "    \n",
    "\n",
    "#Saving the Corpus\n",
    "training_corpus_fn = \"Data/\" + input_corpus_filename + \".labeled.training\"\n",
    "testing_corpus_fn = \"Data/\" + input_corpus_filename + \".labeled.testing\"\n",
    "\n",
    "\n",
    "def save_corpus(data,filename):\n",
    "    out_file = open(filename,\"w\")\n",
    "    for instance in data:\n",
    "        words = \" \".join(instance[0])\n",
    "        label = str(instance[1])\n",
    "        out = words + \"|\" + label + \"\\n\"\n",
    "        out_file.write(out)\n",
    "    out_file.close()\n",
    "\n",
    "#save_corpus(labeled_sentences_train, training_corpus_fn)\n",
    "#save_corpus(labeled_sentences_test, testing_corpus_fn)\n",
    "\n",
    "loaded_corpus = True\n",
    "\n",
    "print(\"Done, you now have {} train instances and {} test instancess:\".format(len(labeled_sentences_train),len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively load an existing corpus\n",
    "loaded_corpus = True\n",
    "if loaded_corpus == False:\n",
    "    #Put your corpus filename here\n",
    "    input_corpus_filename = \"mini.toy\"\n",
    "    \n",
    "    training_corpus_fn = \"Data/\" + input_corpus_filename + \".labeled.training\"\n",
    "    testing_corpus_fn = \"Data/\" + input_corpus_filename + \".labeled.testing\"\n",
    "\n",
    "    def load_corpus(filename):\n",
    "        in_file = open(filename,\"r\")\n",
    "        labeled_data = []\n",
    "        for line in in_file.readlines():\n",
    "            words_str , label = line.rstrip().split(\"|\")\n",
    "            words_list = words_str.split(\" \")\n",
    "            instance = [words_list, int(label)]\n",
    "            labeled_data.append(instance)\n",
    "        return labeled_data\n",
    "\n",
    "    labeled_sentences_train = load_corpus(training_corpus_fn)\n",
    "    labeled_sentences_test = load_corpus(testing_corpus_fn)\n",
    "\n",
    "    print(\"Done, you now have {} train instances and {} test instancess:\".format(len(labeled_sentences_train),len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        linguo.hstate = linguo.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:915.3221575738862\n",
      "1:698.9492537204642\n",
      "2:554.0780851221061\n",
      "3:474.1894234545616\n",
      "4:361.24798116292686\n"
     ]
    }
   ],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "voc_size = len(word_to_ix)\n",
    "hidden_dim = 32\n",
    "epochs = 5\n",
    "linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "optimizer = optim.SGD(linguo.parameters(),lr=0.1)\n",
    "loss_function = nn.NLLLoss()\n",
    "learning_rate=0.1\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(labeled_sentences_train)\n",
    "    for data, label in labeled_sentences_train:\n",
    "        # Restart gradient\n",
    "        linguo.zero_grad()\n",
    "        \n",
    "        \n",
    "        # Run model\n",
    "        in_sentence = prepare_input(word_to_ix,data)\n",
    "        target = autograd.Variable(torch.LongTensor([label]))\n",
    "        prediction = linguo(in_sentence)\n",
    "        #Calculate loss and backpropagate\n",
    "        \n",
    "        #Squared Loss\n",
    "        #loss = torch.pow(target-prediction.view(1),2)\n",
    "        loss = loss_function(prediction,target) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #for parameter in linguo.parameters():\n",
    "        #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "        epoch_loss += loss.data[0]\n",
    "    print(\"{}:{}\".format(i,epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: euro.mini\n",
      "Ngram-Order: 6 \n",
      "Embedding dimension: 32\n",
      "LSTM dimension: 32\n",
      "Hidden Dimension: 32\n",
      "Number of Epochs: 5\n",
      "Final loss:361.24798116292686\n",
      "Accuracy: 0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "salads =[]\n",
    "for testcase in labeled_sentences_test:\n",
    "    prepared_inputs = prepare_input(word_to_ix, testcase[0] )\n",
    "    prediction_vec = linguo(prepared_inputs).view(2)\n",
    "    if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    if prediction == testcase[1]:\n",
    "        correct += 1\n",
    "\n",
    "#Summary:        \n",
    "outtable =\"\"\"Corpus: {corpus}\n",
    "Ngram-Order: {n} \n",
    "Embedding dimension: {embed}\n",
    "LSTM dimension: {lstm}\n",
    "Hidden Dimension: {hidden}\n",
    "Number of Epochs: {epoch}\n",
    "Final loss:{loss}\"\"\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs,\n",
    "                            loss= epoch_loss,\n",
    "                            n=ngram_order)\n",
    "print (outtable)\n",
    "print(\"Accuracy: {}\".format(correct/len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: euro.mini\n",
    "Ngram-Order: 4 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:504.7399406191398\n",
    "Accuracy: 0.7973684210526316\n",
    "\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 10\n",
    "Final loss:232.77821091443258\n",
    "Accuracy: 0.6868421052631579\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:522.7681364201596\n",
    "Accuracy: 0.7236842105263158\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 2 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:363.4900658682709\n",
    "Accuracy: 0.8184210526315789\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 6 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:361.24798116292686\n",
    "Accuracy: 0.8552631578947368\n",
    "\n",
    "\n",
    "\n",
    "Type           | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy | \n",
    ":--------------|:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 51     | 0.97    |\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |\n",
    "Gram Vs Unigram|euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |\n",
    "Gram Vs Bigram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 718    | 0.81    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 840.8  | 0.67    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 673    | 0.75    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 490.9  | 0.75    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "salads =[]\n",
    "for testcase in labeled_sentences_test:\n",
    "    prepared_inputs = prepare_input(word_to_ix, testcase[0] )\n",
    "    prediction_vec = linguo(prepared_inputs).view(2)\n",
    "    if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    if prediction == testcase[1]:\n",
    "        correct += 1\n",
    "\n",
    "#Summary:        \n",
    "outtable =\"\"\"Corpus: {corpus}\n",
    "Embedding dimension: {embed}\n",
    "LSTM dimension: {lstm}\n",
    "Hidden Dimension: {hidden}\n",
    "Number of Epochs: {epoch}\n",
    "Final loss:{loss}\"\"\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs,\n",
    "                            loss= epoch_loss)\n",
    "print (outtable)\n",
    "print(\"Accuracy: {}\".format(correct/len(labeled_sentences_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reminder for the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "modelfilename= \"Models/{corpus}.{embed}emb.{lstm}lstm.{hidden}hid.{epoch}ep.model\".format(\n",
    "                            corpus = input_corpus_filename,\n",
    "                            embed= embed_dim,\n",
    "                            lstm= lstm_dim,\n",
    "                            hidden= hidden_dim,\n",
    "                            epoch= epochs)\n",
    "torch.save(linguo.state_dict(), modelfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section reserver to save the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examplefile = open(\"example\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "fresh_instances = labeled_sentences_train[:]\n",
    "# Get batches ready\n",
    "num_batches = math.floor(len(fresh_instances)/batch_size)\n",
    "batches = []\n",
    "for i in range(num_batches):\n",
    "    batch_data = []\n",
    "    batch_labels = []\n",
    "    for j in range(batch_size):\n",
    "        instance = fresh_instances.pop()\n",
    "        data_tensor = prepare_input(word_to_ix, instance[0])\n",
    "        batch_data.append(data_tensor)\n",
    "        print(data_tensor.size())\n",
    "        batch_labels.append(int(instance[1]))\n",
    "    batch_var = torch.stack(batch_data)\n",
    "    batch_labels_var = autograd.Variable(torch.IntTensor(batch_labels))\n",
    "    batches.append(batch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "corp = [[\"Yo\",\"como\",\"mucha\",\"pizza\"], [\"Yo\",\"tengo\",\"mucha\",\"hambre\"]]\n",
    "\n",
    "corp[0].insert(0,\"#\")\n",
    "corp[0].insert(len(corp[0]),\"#\")\n",
    "\n",
    "fdist = extract_ngram_freq(corp,2)\n",
    "generateWSNgram(fdist,3,0.1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstWord = choice(fwvocab,1,fwfreqs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
