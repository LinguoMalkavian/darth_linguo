{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the capacity of LSTM with word embeddings to detect syntactic structure via binary decision tasks\n",
    "## Presented by Pablo Gonzaalez Martinez\n",
    "\n",
    "This notebook contains the core code used for the experiments presented in the final project: \"Was that a sentence? Exploring the capacity of LSTM with word embeddings to detect syntactic structure via binary decision tasks\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing tokenizes and randomizes the order of the sentences, it then extracts low frequency items and replaces those tokens with the unknown token. Data generation produces the n-gram noise sentences to be used in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    tokenizer = MosesTokenizer()\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in real_text]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<5:\n",
    "        length = 5\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the architecture of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        self.hstate = self.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "def train_model(train_data,\n",
    "                embed_dim,\n",
    "                lstm_dim,\n",
    "                hidden_dim,\n",
    "                word_to_ix,\n",
    "                epochs,\n",
    "                learning_rate):\n",
    "    voc_size = len(word_to_ix)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word_to_ix,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"\\t Epoch{}:{}\".format(i,epoch_loss))\n",
    "    return linguo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing, testing\n",
    "def test_model(test_data,model,word2id):\n",
    "    correct = 0.0\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for testcase in test_data:\n",
    "        target = testcase[1]\n",
    "        prepared_inputs = prepare_input(word2id, testcase[0] )\n",
    "        prediction_vec = model(prepared_inputs).view(2)\n",
    "        if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if prediction == testcase[1]:\n",
    "            correct += 1\n",
    "            if target == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                fn +=1\n",
    "            else:\n",
    "                fp +=1\n",
    "                \n",
    "    # Compile results\n",
    "    accuracy = correct/len(test_data)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    fmeasure = 2*tp / (2*tp+fp+fn) \n",
    "    results = {\"accuracy\":accuracy,\n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "               \"fmeasure\":fmeasure,\n",
    "              \"tp\":tp,\n",
    "              \"tn\":tn,\n",
    "              \"fp\":fp,\n",
    "              \"fn\":fn}\n",
    "    return results\n",
    "\n",
    "def average_results(result_list):\n",
    "    total = len(result_list)\n",
    "    averaged =defaultdict(float)\n",
    "    for report in result_list:\n",
    "        for item in report:\n",
    "            averaged[item] += report[item]\n",
    "    for item in averaged:\n",
    "        averaged[item] = averaged[item]/total\n",
    "    return averaged\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 85 sentences, 15 were dumped, among which 10 interogatives or exclamatives\n",
      "Your corpus has 85 grammatical sentences\n",
      "Word salad data has been generated for order 1\n",
      "\t85 word salads generated in 0.005 seconds\n",
      "Starting training on fold 1 for 1-grams...\n",
      "\t Epoch0:64.45398574974388\n",
      "\t Epoch1:25.43276189523749\n",
      "\t Epoch2:2.1941675296984613\n",
      "Training finished in 9.7945 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.29378199577331543 seconds\n",
      "\\Accuracy is 1.0\n",
      "Starting training on fold 2 for 1-grams...\n",
      "\t Epoch0:71.56373368576169\n",
      "\t Epoch1:24.483111983980052\n",
      "\t Epoch2:12.932864377798978\n",
      "Training finished in 10.7414 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.25443482398986816 seconds\n",
      "\\Accuracy is 0.9705882352941176\n",
      "Starting training on fold 3 for 1-grams...\n",
      "\t Epoch0:69.98426482081413\n",
      "\t Epoch1:18.129805100150406\n",
      "\t Epoch2:3.1222621232736856\n",
      "Training finished in 10.2811 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.29058027267456055 seconds\n",
      "\\Accuracy is 0.9705882352941176\n",
      "Starting training on fold 4 for 1-grams...\n",
      "\t Epoch0:60.84535718662664\n",
      "\t Epoch1:11.502233231440187\n",
      "\t Epoch2:9.810259003890678\n",
      "Training finished in 10.7307 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.25246119499206543 seconds\n",
      "\\Accuracy is 0.9117647058823529\n",
      "Starting training on fold 5 for 1-grams...\n",
      "\t Epoch0:68.18244015425444\n",
      "\t Epoch1:23.921204273123294\n",
      "\t Epoch2:10.064078910159878\n",
      "Training finished in 10.9260 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.25781702995300293 seconds\n",
      "\\Accuracy is 0.9411764705882353\n",
      "Starting training on fold 6 for 1-grams...\n",
      "\t Epoch0:70.48173539340496\n",
      "\t Epoch1:18.9926375313662\n",
      "\t Epoch2:13.80001010664273\n",
      "Training finished in 10.9248 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.2397620677947998 seconds\n",
      "\\Accuracy is 0.9411764705882353\n",
      "Starting training on fold 7 for 1-grams...\n",
      "\t Epoch0:76.66992633976042\n",
      "\t Epoch1:42.282713409978896\n",
      "\t Epoch2:19.316913287475472\n",
      "Training finished in 11.2480 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.19730114936828613 seconds\n",
      "\\Accuracy is 0.8529411764705882\n",
      "Starting training on fold 8 for 1-grams...\n",
      "\t Epoch0:82.86032808944583\n",
      "\t Epoch1:31.806341256946325\n",
      "\t Epoch2:8.676558225881308\n",
      "Training finished in 10.6006 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.29062581062316895 seconds\n",
      "\\Accuracy is 0.9705882352941176\n",
      "Starting training on fold 9 for 1-grams...\n",
      "\t Epoch0:76.68939528800547\n",
      "\t Epoch1:38.31652423646301\n",
      "\t Epoch2:18.2209753759671\n",
      "Training finished in 11.6311 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.2724883556365967 seconds\n",
      "\\Accuracy is 1.0\n",
      "Starting training on fold 10 for 1-grams...\n",
      "\t Epoch0:78.98646733537316\n",
      "\t Epoch1:33.33704903908074\n",
      "\t Epoch2:25.404794384958223\n",
      "Training finished in 11.0303 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.3098433017730713 seconds\n",
      "\\Accuracy is 0.9411764705882353\n",
      "Results are in for 1-grams\n",
      "\tFinished 10 folds in 110.5850 s\n",
      "\tAverage accuracy is:0.95\n",
      "\tAverage F measure is:0.9476335940045617\n",
      "Word salad data has been generated for order 2\n",
      "\t85 word salads generated in 0.064 seconds\n",
      "Starting training on fold 1 for 2-grams...\n",
      "\t Epoch0:94.45853047072887\n",
      "\t Epoch1:85.03723755478859\n",
      "\t Epoch2:81.489443205297\n",
      "Training finished in 12.3514 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.2702939510345459 seconds\n",
      "\\Accuracy is 0.5294117647058824\n",
      "Starting training on fold 2 for 2-grams...\n",
      "\t Epoch0:97.58366346359253\n",
      "\t Epoch1:87.01885628700256\n",
      "\t Epoch2:80.7812700048089\n",
      "Training finished in 11.9212 seconds, starting testing...\n",
      "...\n",
      "Testing finished in 0.35978007316589355 seconds\n",
      "\\Accuracy is 0.5588235294117647\n",
      "Starting training on fold 3 for 2-grams...\n",
      "\t Epoch0:96.03125524520874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ae848b68d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                             \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                             learning_rate)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training finished in {:.4f} seconds, starting testing...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-878c8bd6c2bb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, embed_dim, lstm_dim, hidden_dim, word_to_ix, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#loss = torch.pow(target-prediction.view(1),2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#for parameter in linguo.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell runs the full experiment with cross validation\n",
    "\n",
    "# Modify parameters here\n",
    "corpus_name = \"euro.micro\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 10\n",
    "train_proportion = 0.8\n",
    "embed_dim = 32\n",
    "lstm_dim = 32\n",
    "hidden_dim = 32\n",
    "epochs = 3\n",
    "learning_rate=0.1\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in prepro_gram]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "full_results = []\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(1,max_ngram+1):\n",
    "    t2 = time()\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in range(nsal)]\n",
    "    \n",
    "    labeled_g = [[sentence,1] for sentence in prepro_gram]\n",
    "    labeled_ws = [[sentence,0] for sentence in word_salads]\n",
    "    \n",
    "    message = \"Word salad data has been generated for order {}\".format(n)\n",
    "    print(message)\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,\n",
    "                                                                      te)\n",
    "    print(message)\n",
    "    message= \"Starting experiment on {}-grams ...\".format(n)\n",
    "    \n",
    "    result_list = []\n",
    "    # Iterate over the number of folds\n",
    "    for fold in range(folds):\n",
    "        t1 = time()\n",
    "        message = \"Starting training on fold {} for {}-grams...\".format(fold+1,n)\n",
    "        print(message)\n",
    "        # Shuffle and split data\n",
    "        random.shuffle(labeled_g)\n",
    "        random.shuffle(labeled_ws)\n",
    "        cutoff = math.floor(train_proportion * len(labeled_g))\n",
    "        train_g, test_g = labeled_g[:cutoff],labeled_g[cutoff:]\n",
    "        train_ws,test_ws = labeled_ws[:cutoff],labeled_ws[cutoff:]\n",
    "        \n",
    "        train_data = train_g + train_ws\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        test_data = test_g + test_ws\n",
    "        random.shuffle(test_data)\n",
    "        \n",
    "        # Train the Model\n",
    "        model = train_model(train_data,\n",
    "                            embed_dim,\n",
    "                            lstm_dim,\n",
    "                            hidden_dim,\n",
    "                            word2id,\n",
    "                            epochs,\n",
    "                            learning_rate)\n",
    "        te = time()-t1\n",
    "        message = \"Training finished in {:.4f} seconds, starting testing...\".format(te)\n",
    "        print(message)\n",
    "        print(\"...\")\n",
    "        t1 = time()\n",
    "        # Test the Model\n",
    "        fold_results = test_model(test_data,model,word2id)\n",
    "        result_list.append(fold_results)\n",
    "        te = time()-t1\n",
    "        message = \"Testing finished in {} seconds\".format(te)\n",
    "        print(message)\n",
    "        message = \"\\Accuracy is {}\".format(fold_results['accuracy'])\n",
    "        print(message)\n",
    "        \n",
    "    order_results = average_results(result_list)\n",
    "    te2 = time()- t2\n",
    "    message=\"Results are in for {}-grams\".format(n)\n",
    "    print(message)\n",
    "    message=\"\\tFinished {} folds in {:.4f} s\".format(folds,te2)\n",
    "    print(message)\n",
    "    message=\"\\tAverage accuracy is:{}\".format(order_results[\"accuracy\"])\n",
    "    print(message)\n",
    "    message=\"\\tAverage F measure is:{}\".format(order_results[\"fmeasure\"])\n",
    "    print(message)\n",
    "    full_results.append(order_results)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
