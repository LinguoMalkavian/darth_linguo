{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a first attempt at a rudimentary sentence grammaticality classifier \n",
    "# It is meant as a feasibility test for the larger experiment and as a learning\n",
    "# Experience for future endeavours\n",
    "\n",
    "# Standard pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "#NLTK modules\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "#other utilities\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from string import capwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for importing, preprocessing and generating data\n",
    "\n",
    "\n",
    "def load_grammatical_corpus(input_corpus_filename):\n",
    "    input_corpus_path = \"Data/\"+input_corpus_filename\n",
    "    in_file = open(input_corpus_path, \"r\")\n",
    "    real_text = []\n",
    "    numlines = 0\n",
    "    inter_excl=0\n",
    "    for line in in_file.readlines():\n",
    "        #Keep only sentences, those have a period at the end (is support for ? and ! needed??)\n",
    "        if line.strip() !=\"\":\n",
    "            if line.strip()[-1] == \".\":\n",
    "                real_text.append(line.strip())\n",
    "            elif line.strip()[-1] == \"?\" or line.strip()[-1] == \"!\":\n",
    "                inter_excl +=1\n",
    "        numlines+=1\n",
    "\n",
    "    print(\"Full corpus has {} sentences, {} were dumped, among which {} interogatives or exclamatives\".format(\n",
    "                                len(real_text),numlines-len(real_text),inter_excl))\n",
    "\n",
    "    random.shuffle(real_text)\n",
    "    # Process the input sentences (for tokenization, tokenizer sucks otherwise)\n",
    "    tokenizer = MosesTokenizer()\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in real_text]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# Method to extract:\n",
    "# The word2idx, a dictionary from vocabulary words to unique integers\n",
    "# The hapaxes, a list with words whos total count in the corpus is less than the threshold\n",
    "# Vocabulary and probdist are also generated to be used exclusively in the unigram case\n",
    "def get_vocabulary(sentences,hap_threshold):\n",
    "    counts = defaultdict(int)\n",
    "    total = 0.0\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token != \".\":\n",
    "                counts[token.lower()] +=1\n",
    "                total += 1\n",
    "    hapaxes = []\n",
    "    counts[\"#unk\"]=0\n",
    "    # Identify hapaxes, count them for smoothing\n",
    "    for key in counts:\n",
    "        if counts[key] <= hap_threshold:\n",
    "            counts[\"#unk\"] += 1\n",
    "            hapaxes.append(key)\n",
    "    #Remove them from the count\n",
    "    for hapax in hapaxes:\n",
    "        counts.pop(hapax)\n",
    "    #Consolidate vocabulary and word ids\n",
    "    vocabulary = []\n",
    "    probdist = []\n",
    "    for key in counts:\n",
    "        vocabulary.append(key)\n",
    "        probdist.append(counts[key])\n",
    "    \n",
    "    #Define the vocabulary and word ids\n",
    "    vocabulary.append(\".\")\n",
    "    word_to_ix = {}\n",
    "    for word in vocabulary:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "    return word_to_ix, hapaxes, vocabulary, probdist, counts\n",
    "    \n",
    "\n",
    "# Method to extract n-gram frequencies from the corpus\n",
    "# as well as length statistics\n",
    "# Corpus is a list of sentences, each sentence represented by a list of tokens\n",
    "def extract_ngram_freq(corpus,order):\n",
    "    n_frequencies = defaultdict(lambda:defaultdict(int))\n",
    "    for sentence in corpus:\n",
    "        for _ in range(order-1):\n",
    "            sentence.insert(0,\"#\")\n",
    "        for ini in range(len(sentence) - order ):\n",
    "            prefix = \" \".join(sentence[ini:ini+order-1])\n",
    "            target = sentence[ini+order-1]\n",
    "            n_frequencies[prefix][target]+= 1\n",
    "    return n_frequencies\n",
    "\n",
    "\n",
    "# Method to replace hapaxes by the unk token in the corpus\n",
    "def token_replacement(sentences, hapaxes):\n",
    "    # Takes a list of tokenized sentences \n",
    "    # Returns a list of sentences, each of which is a list of words (str)\n",
    "    # Words specified in hapaxes are replaced by UNK\n",
    "    cleaned = []\n",
    "    for sentence in sentences:\n",
    "        this_sentence = []\n",
    "        for token in sentence:\n",
    "            if token.lower() in hapaxes:\n",
    "                this_sentence.append(\"#unk\")\n",
    "            else:\n",
    "                this_sentence.append(token)\n",
    "        cleaned.append(this_sentence)\n",
    "    return cleaned\n",
    "\n",
    "def generateWSuni(vocab, probdist, avg_length,sd):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<6:\n",
    "        length = 6\n",
    "    #Draw the words\n",
    "    draw= choice(vocab, length, probdist).tolist()\n",
    "    #Assemble the sentence\n",
    "    sentence = [capwords(draw.pop(0))]\n",
    "    while draw:\n",
    "        next_word = draw.pop(0)\n",
    "        #special case for punctuation that needs to be closed\n",
    "        if next_word in [\"(\",\"«\"]:\n",
    "            try:\n",
    "                sentence.append(next_word) \n",
    "                sentence.append(draw.pop(0))\n",
    "                closing = \"\"\n",
    "                if next_word == \"(\":\n",
    "                    closing = \")\"\n",
    "                elif next_word == \"«\":\n",
    "                    closing = \"»\"\n",
    "                draw.insert(random.randint(0,len(draw)),closing)\n",
    "            except IndexError:\n",
    "                break\n",
    "        elif next_word not in [\")\",\"»\"]:\n",
    "            sentence.append(next_word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "def generateWSNgram(n_frequencies, avg_length,sd,order,unicounts):\n",
    "    # Method to generate one word salad sentence usin unigram distribution\n",
    "    # Vocab is a list of vocabulary words\n",
    "    # probdist contains the probabilities of vocabulary words in same order\n",
    "    # avg_length is the average length of sentences\n",
    "    # sd is the standar deviation for the legths of sentences\n",
    "    \n",
    "    #Draw the length\n",
    "    length= math.floor(random.gauss(avg_length, sd))\n",
    "    if length<5:\n",
    "        length = 5\n",
    "    \n",
    "    sentence = [\"#\"]*(order-1)\n",
    "    for i in range(length+order-1):\n",
    "        prefix = \" \".join(sentence[-(order-1):])\n",
    "        try:\n",
    "            vocab, freqs = zip(*n_frequencies[prefix].items())\n",
    "            word = choice(vocab,1,freqs)[0]\n",
    "            sentence.append(word)\n",
    "        except:\n",
    "            vocab, freqs = zip(*unicounts.items())\n",
    "            word = choice(vocab, 1, freqs)[0]\n",
    "            sentence.append(word)\n",
    "    sentence.append(\".\")\n",
    "    return sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the Neural network\n",
    "\n",
    "\n",
    "class Linguo(nn.Module):\n",
    "    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):\n",
    "        super(Linguo,self).__init__()\n",
    "        # Store the hidden layer dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        #Define hidden linear layer\n",
    "        self.hidden2dec = nn.Linear(hidden_dim, 2)\n",
    "        #Define the hidden state\n",
    "        self.hstate = self.init_hstate()\n",
    "        \n",
    "    def forward(self, inputsentence):\n",
    "        self.hstate = self.init_hstate()\n",
    "        embeds = self.word_embeddings(inputsentence)\n",
    "        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)\n",
    "        decision_lin = self.hidden2dec(lstm_out[-1])\n",
    "        #print(decision_lin)\n",
    "        decision_fin = F.log_softmax(decision_lin)\n",
    "        return decision_fin\n",
    "        \n",
    "        \n",
    "    def init_hstate(self):\n",
    "        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim)) \n",
    "        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))\n",
    "        hidden_state = (var1, var2)\n",
    "        return hidden_state\n",
    "        \n",
    "        \n",
    "def prepare_input(word_to_ix, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word.lower()])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"#unk\"])\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time! Cue Eye of the Tiger\n",
    "\n",
    "def train_model(train_data,\n",
    "                embed_dim,\n",
    "                lstm_dim,\n",
    "                hidden_dim,\n",
    "                word_to_ix,\n",
    "                epochs,\n",
    "                learning_rate):\n",
    "    voc_size = len(word_to_ix)\n",
    "    # Initialize model\n",
    "    linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim) \n",
    "    optimizer = optim.SGD(linguo.parameters(),lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        random.shuffle(train_data)\n",
    "        for data, label in train_data:\n",
    "            # Restart gradient\n",
    "            linguo.zero_grad()\n",
    "            # Run model\n",
    "            in_sentence = prepare_input(word_to_ix,data)\n",
    "            target = autograd.Variable(torch.LongTensor([label]))\n",
    "            prediction = linguo(in_sentence)\n",
    "            #Calculate loss and backpropagate\n",
    "\n",
    "            #Squared Loss\n",
    "            #loss = torch.pow(target-prediction.view(1),2)\n",
    "            loss = loss_function(prediction,target) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #for parameter in linguo.parameters():\n",
    "            #   parameter.data.sub_(parameter.grad.data*learning_rate)\n",
    "            epoch_loss += loss.data[0]\n",
    "        print(\"\\t Epoch{}:{}\".format(i,epoch_loss))\n",
    "    return linguo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, testing\n",
    "def test_model(test_data,model,word2id):\n",
    "    correct = 0.0\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for testcase in test_data:\n",
    "        target = testcase[1]\n",
    "        prepared_inputs = prepare_input(word2id, testcase[0] )\n",
    "        prediction_vec = model(prepared_inputs).view(2)\n",
    "        if prediction_vec.data[0] > prediction_vec.data[1]:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if prediction == testcase[1]:\n",
    "            correct += 1\n",
    "            if target == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                tn+=1\n",
    "        else:\n",
    "            if target == 1:\n",
    "                fn +=1\n",
    "            else:\n",
    "                fp +=1\n",
    "                \n",
    "    # Compile results\n",
    "    accuracy = correct/len(test_data)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    fmeasure = 2*tp / (2*tp+fp+fn) \n",
    "    results = {\"accuracy\":accuracy,\n",
    "               \"precision\":precision,\n",
    "               \"recall\":recall,\n",
    "               \"fmeasure\":fmeasure,\n",
    "              \"tp\":tp,\n",
    "              \"tn\":tn,\n",
    "              \"fp\":fp,\n",
    "              \"fn\":fn}\n",
    "    return results\n",
    "\n",
    "def average_results(result_list):\n",
    "    total = len(result_list)\n",
    "    averaged =defaultdict(float)\n",
    "    for report in result_list:\n",
    "        for item in report:\n",
    "            averaged[item] += report[item]\n",
    "    for item in averaged:\n",
    "        averaged[item] = averaged[item]/total\n",
    "    return averaged\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus has 947 sentences, 53 were dumped, among which 33 interogatives or exclamatives\n",
      "Your corpus has 947 grammatical sentences\n",
      "Word salad data has been generated for order 2\n",
      "\t947 word salads generated in 0.794 seconds\n",
      "Starting training on fold 1 for 2-grams...\n",
      "\t Epoch0:989.4511688500643\n",
      "\t Epoch1:855.1364701231942\n",
      "\t Epoch2:701.8174962145276\n",
      "Training finished in 106.9201 seconds, starting testing...\n",
      "Starting training on fold 2 for 2-grams...\n",
      "\t Epoch0:1019.8904156461358\n",
      "\t Epoch1:906.4229545434937\n",
      "\t Epoch2:734.5915766789112\n",
      "Training finished in 107.5987 seconds, starting testing...\n",
      "Starting training on fold 3 for 2-grams...\n",
      "\t Epoch0:994.7536506727338\n",
      "\t Epoch1:902.5262282602489\n",
      "\t Epoch2:769.1616146368906\n",
      "Training finished in 109.6061 seconds, starting testing...\n",
      "Starting training on fold 4 for 2-grams...\n",
      "\t Epoch0:1005.6755542829633\n",
      "\t Epoch1:935.2497241422534\n",
      "\t Epoch2:805.140499336645\n",
      "Training finished in 109.0643 seconds, starting testing...\n",
      "Starting training on fold 5 for 2-grams...\n",
      "\t Epoch0:997.8149748258293\n",
      "\t Epoch1:810.9120978782885\n",
      "\t Epoch2:611.4086493288632\n",
      "Training finished in 109.7701 seconds, starting testing...\n",
      "Starting training on fold 6 for 2-grams...\n",
      "\t Epoch0:998.4111733026803\n",
      "\t Epoch1:861.7937068529427\n",
      "\t Epoch2:692.2873147714417\n",
      "Training finished in 108.7094 seconds, starting testing...\n",
      "Starting training on fold 7 for 2-grams...\n",
      "\t Epoch0:1018.9812762327492\n",
      "\t Epoch1:838.5907407514751\n",
      "\t Epoch2:719.6729636617238\n",
      "Training finished in 110.4571 seconds, starting testing...\n",
      "Starting training on fold 8 for 2-grams...\n",
      "\t Epoch0:1014.3764255195856\n",
      "\t Epoch1:930.4308128878474\n",
      "\t Epoch2:848.5271038096398\n",
      "Training finished in 108.1101 seconds, starting testing...\n",
      "Starting training on fold 9 for 2-grams...\n",
      "\t Epoch0:998.9980729371309\n",
      "\t Epoch1:903.4984672218561\n",
      "\t Epoch2:771.0041804385837\n",
      "Training finished in 109.6490 seconds, starting testing...\n",
      "Starting training on fold 10 for 2-grams...\n",
      "\t Epoch0:1006.0048832073808\n",
      "\t Epoch1:896.0760003123432\n",
      "\t Epoch2:790.586819158867\n",
      "Training finished in 110.8636 seconds, starting testing...\n",
      "Results are in for 2-grams\n",
      "\tFinished 10 folds in 1115.6622 s\n",
      "\tAverage accuracy is:0.7321052631578947\n",
      "\tAverage F measure is:0.7134444928677581\n",
      "Word salad data has been generated for order 3\n",
      "\t947 word salads generated in 4.125 seconds\n",
      "Starting training on fold 1 for 3-grams...\n",
      "\t Epoch0:1014.5794398710132\n",
      "\t Epoch1:882.1209167893976\n",
      "\t Epoch2:752.0679115096573\n",
      "Training finished in 114.0215 seconds, starting testing...\n",
      "Starting training on fold 2 for 3-grams...\n",
      "\t Epoch0:1009.7582023367286\n",
      "\t Epoch1:906.0393972434103\n",
      "\t Epoch2:800.4929399159737\n",
      "Training finished in 115.0568 seconds, starting testing...\n",
      "Starting training on fold 3 for 3-grams...\n",
      "\t Epoch0:997.0672943666577\n",
      "\t Epoch1:895.9373228866607\n",
      "\t Epoch2:804.6504730065353\n",
      "Training finished in 115.2525 seconds, starting testing...\n",
      "Starting training on fold 4 for 3-grams...\n",
      "\t Epoch0:983.6325833573937\n",
      "\t Epoch1:895.0385871231556\n",
      "\t Epoch2:783.0179750695825\n",
      "Training finished in 115.6589 seconds, starting testing...\n",
      "Starting training on fold 5 for 3-grams...\n",
      "\t Epoch0:1018.8805878013372\n",
      "\t Epoch1:941.8657546676695\n",
      "\t Epoch2:878.4617994744331\n",
      "Training finished in 117.6309 seconds, starting testing...\n",
      "Starting training on fold 6 for 3-grams...\n"
     ]
    }
   ],
   "source": [
    "# This cell runs the full experiment with cross validation\n",
    "\n",
    "# Modify parameters here\n",
    "corpus_name = \"euro.mini\"\n",
    "max_ngram = 6\n",
    "hap_thresh = 1\n",
    "folds = 10\n",
    "train_proportion = 0.8\n",
    "embed_dim = 20\n",
    "lstm_dim = 20\n",
    "hidden_dim = 20\n",
    "epochs = 3\n",
    "learning_rate=0.1\n",
    "\n",
    "#Load and preprocess the grammatical part of the corpus\n",
    "t1 = time()\n",
    "\n",
    "corpus = load_grammatical_corpus(corpus_name)\n",
    "word2id, hapaxes, vocab, probdist, ucounts = get_vocabulary(corpus,hap_thresh)\n",
    "prepro_gram = token_replacement(corpus,hapaxes)\n",
    "message = \"Your corpus has {sent} grammatical sentences\".format(\n",
    "                                                    sent=len(prepro_gram))\n",
    "print(message)\n",
    "message = \"Grammatical corpus loaded in {:.3f} seconds\".format(time()-t1)\n",
    "\n",
    "# Get sentence length statistics\n",
    "lengths= [len(sent) for sent in prepro_gram]\n",
    "avg_sent_length = np.mean(lengths)\n",
    "length_sd = np.std(lengths)\n",
    "\n",
    "full_results = []\n",
    "# Run for each n, with x-fold cross validation\n",
    "for n in range(2,max_ngram+1):\n",
    "    t2 = time()\n",
    "    # Generate the word salads\n",
    "    message = \"Generating word salads of order {}...\".format(n)\n",
    "    t1= time()\n",
    "    nsal = len(prepro_gram)\n",
    "    if n == 1:\n",
    "        word_salads = [generateWSuni(vocab,\n",
    "                                    probdist,\n",
    "                                    avg_sent_length,\n",
    "                                    length_sd)\n",
    "                      for _ in range(nsal)]\n",
    "    else:\n",
    "        n_freqs = extract_ngram_freq(prepro_gram,n)\n",
    "        word_salads = [generateWSNgram(n_freqs,\n",
    "                                     avg_sent_length,\n",
    "                                     length_sd,\n",
    "                                     n,\n",
    "                                     ucounts\n",
    "                                     )\n",
    "                       for _ in range(nsal)]\n",
    "    \n",
    "    labeled_g = [[sentence,1] for sentence in prepro_gram]\n",
    "    labeled_ws = [[sentence,0] for sentence in word_salads]\n",
    "    \n",
    "    message = \"Word salad data has been generated for order {}\".format(n)\n",
    "    print(message)\n",
    "    te = time()- t1\n",
    "    message = \"\\t{} word salads generated in {:.3f} seconds\".format(nsal,\n",
    "                                                                      te)\n",
    "    print(message)\n",
    "    message= \"Starting experiment on {}-grams ...\".format(n)\n",
    "    \n",
    "    result_list = []\n",
    "    # Iterate over the number of folds\n",
    "    for fold in range(folds):\n",
    "        t1 = time()\n",
    "        message = \"Starting training on fold {} for {}-grams...\".format(fold+1,n)\n",
    "        print(message)\n",
    "        # Shuffle and split data\n",
    "        random.shuffle(labeled_g)\n",
    "        random.shuffle(labeled_ws)\n",
    "        cutoff = math.floor(train_proportion * len(labeled_g))\n",
    "        train_g, test_g = labeled_g[:cutoff],labeled_g[cutoff:]\n",
    "        train_ws,test_ws = labeled_ws[:cutoff],labeled_ws[cutoff:]\n",
    "        \n",
    "        train_data = train_g + train_ws\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        test_data = test_g + test_ws\n",
    "        random.shuffle(test_data)\n",
    "        \n",
    "        # Train the Model\n",
    "        model = train_model(train_data,\n",
    "                            embed_dim,\n",
    "                            lstm_dim,\n",
    "                            hidden_dim,\n",
    "                            word2id,\n",
    "                            epochs,\n",
    "                            learning_rate)\n",
    "        te = time()-t1\n",
    "        message = \"Training finished in {:.4f} seconds, starting testing...\".format(te)\n",
    "        print(message)\n",
    "        t1 = time()\n",
    "        # Test the Model\n",
    "        fold_results = test_model(test_data,model,word2id)\n",
    "        result_list.append(fold_results)\n",
    "        te = time()-t1\n",
    "        message = \"Testing finished in {} seconds\".format(te)\n",
    "        \n",
    "        \n",
    "    order_results = average_results(result_list)\n",
    "    te2 = time()- t2\n",
    "    message=\"Results are in for {}-grams\".format(n)\n",
    "    print(message)\n",
    "    message=\"\\tFinished {} folds in {:.4f} s\".format(folds,te2)\n",
    "    print(message)\n",
    "    message=\"\\tAverage accuracy is:{}\".format(order_results[\"accuracy\"])\n",
    "    print(message)\n",
    "    message=\"\\tAverage F measure is:{}\".format(order_results[\"fmeasure\"])\n",
    "    print(message)\n",
    "    full_results.append(order_results)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'accuracy': 0.7742105263157895,\n",
       "             'fmeasure': 0.7733546517277098,\n",
       "             'fn': 39.0,\n",
       "             'fp': 46.8,\n",
       "             'precision': 0.770421382714067,\n",
       "             'recall': 0.7947368421052632,\n",
       "             'tn': 143.2,\n",
       "             'tp': 151.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_results = average_results(result_list)\n",
    "order_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus: euro.mini\n",
    "Ngram-Order: 4 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:504.7399406191398\n",
    "Accuracy: 0.7973684210526316\n",
    "\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 10\n",
    "Final loss:232.77821091443258\n",
    "Accuracy: 0.6868421052631579\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 3 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:522.7681364201596\n",
    "Accuracy: 0.7236842105263158\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 2 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:363.4900658682709\n",
    "Accuracy: 0.8184210526315789\n",
    "\n",
    "Corpus: euro.mini\n",
    "Ngram-Order: 6 \n",
    "Embedding dimension: 32\n",
    "LSTM dimension: 32\n",
    "Hidden Dimension: 32\n",
    "Number of Epochs: 5\n",
    "Final loss:361.24798116292686\n",
    "Accuracy: 0.8552631578947368\n",
    "\n",
    "\n",
    "\n",
    "Type           | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy | \n",
    ":--------------|:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 51     | 0.97    |\n",
    "Gram Vs Unigram|euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |\n",
    "Gram Vs Unigram|euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |\n",
    "Gram Vs Bigram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 718    | 0.81    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   3    | 840.8  | 0.67    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 673    | 0.75    |\n",
    "Gram Vs 3-gram |euro.mini  | 1514/380   | 10    | 10   | 10     |   5    | 490.9  | 0.75    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
