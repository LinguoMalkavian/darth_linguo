# This model does the comparison between sentences and word salads
# Generated by drawing from the Unigram distribution

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
# spacy model
import spacy

# other utilities
import numpy as np
from numpy.random import choice
import random
from collections import defaultdict
import math
from string import capwords



# Standard pytorch imports
def main():
    # load Spacy spanish model to handle tokenization of toy data
    nlp = spacy.load('es_core_news_sm')
# In[49]:



# In[45]:


# Alternatively load an existing corpus

if loaded _corpus == False:
    #Put your corpus filename here
    input_corpus_filename = "mini.toy"

    training_corpus_fn = "Data/" + input_corpus_filename + ".labeled.training"
    testing_corpus_fn = "Data/" + input_corpus_filename + ".labeled.testing"

    def load_corpus(filename):
        in_file = open(filename,"r")
        labeled_data = []
        for line in in_file.readlines():
            words_str , label = line.rstrip().split("|")
            words_list = words_str.split(" ")
            instance = [words_list, int(label)]
            labeled_data.append(instance)
        return labeled_data

    labeled_sentences_train1 = load_corpus(training_corpus_fn)
    labeled_sentences_test1 = load_corpus(testing_corpus_fn)

    print("Done, you now have {} train instances and {} test instancess:".format(len(labeled_sentences_train),len(labeled_sentences_test)))


# In[46]:





# In[50]:


# Now we define the Neural network


class Linguo(nn.Module):
    def __init__(self,embedding_dim, vocab_size, lstm_dim , hidden_dim):
        super(Linguo,self).__init__()
        # Store the hidden layer dimension
        self.hidden_dim = hidden_dim
        # Define word embeddings
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # Define LSTM
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        #Define hidden linear layer
        self.hidden2dec = nn.Linear(hidden_dim, 2)
        #Define the hidden state
        self.hstate = self.init_hstate()

    def forward(self, inputsentence):
        linguo.hstate = linguo.init_hstate()
        embeds = self.word_embeddings(inputsentence)
        lstm_out, self.hstate = self.lstm(embeds.view(len(inputsentence),1, -1), self.hstate)
        decision_lin = self.hidden2dec(lstm_out[-1])
        #print(decision_lin)
        decision_fin = F.log_softmax(decision_lin)
        return decision_fin


    def init_hstate(self):
        var1 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))
        var2 = autograd.Variable(torch.zeros(1, 1, self.hidden_dim))
        hidden_state = (var1, var2)
        return hidden_state


def prepare_input(word_to_ix, sentence):
    idxs = []
    for word in sentence:
        if word in word_to_ix:
            idxs.append(word_to_ix[word.lower()])
        else:
            idxs.append(word_to_ix["#unk"])
    tensor = torch.LongTensor(idxs)
    return autograd.Variable(tensor)



# In[51]:


# Training time! Cue Eye of the Tiger
embed_dim = 32
lstm_dim = 64
voc_size = len(word_to_ix)
hidden_dim = 64
epochs = 25
linguo = Linguo(embed_dim, voc_size, lstm_dim, hidden_dim)
optimizer = optim.SGD(linguo.parameters(),lr=0.1)
loss_function = nn.NLLLoss()
learning_rate=0.1

for i in range(epochs):
    epoch_loss = 0
    random.shuffle(labeled_sentences_train)
    for data, label in labeled_sentences_train:
        # Restart gradient
        linguo.zero_grad()


        # Run model
        in_sentence = prepare_input(word_to_ix,data)
        target = autograd.Variable(torch.LongTensor([label]))
        prediction = linguo(in_sentence)
        #Calculate loss and backpropagate

        #Squared Loss
        #loss = torch.pow(target-prediction.view(1),2)
        loss = loss_function(prediction,target)

        loss.backward()
        optimizer.step()
        #for parameter in linguo.parameters():
        #   parameter.data.sub_(parameter.grad.data*learning_rate)
        epoch_loss += loss.data[0]
    print("{}:{}".format(i,epoch_loss))



# In[52]:


correct = 0
salads =[]
for testcase in labeled_sentences_test:
    prepared_inputs = prepare_input(word_to_ix, testcase[0] )
    prediction_vec = linguo(prepared_inputs).view(2)
    if prediction_vec.data[0] > prediction_vec.data[1]:
        prediction = 0
    else:
        prediction = 1
    if prediction == testcase[1]:
        correct += 1

#Summary:
outtable ="""Corpus: {corpus}
Embedding dimension: {embed}
LSTM dimension: {lstm}
Hidden Dimension: {hidden}
Number of Epochs: {epoch}
Final loss:{loss}""".format(
                            corpus = input_corpus_filename,
                            embed= embed_dim,
                            lstm= lstm_dim,
                            hidden= hidden_dim,
                            epoch= epochs,
                            loss= epoch_loss)
print (outtable)
print("Accuracy: {}".format(correct/len(labeled_sentences_test)))


# | Corpus    |Corpus Size | Embed | LSTM | Hidden | Epochs | Loss   |Accuracy |
# |:----------|:----------:|:-----:|:----:|:------:|:------:|:------:|:-------:|
# |euro.mini  | 1514/380   | 32    | 32   | 64     |   50   | 0.034  | 0.98    |
# |euro.toy   | 29730/7434 | 32    | 64   | 64     |   25   | 0.06   | 0.99    |
#

# In[53]:


correct = 0
salads =[]
for testcase in labeled_sentences_test:
    prepared_inputs = prepare_input(word_to_ix, testcase[0] )
    prediction_vec = linguo(prepared_inputs).view(2)
    if prediction_vec.data[0] > prediction_vec.data[1]:
        prediction = 0
    else:
        prediction = 1
    if prediction == testcase[1]:
        correct += 1

#Summary:
outtable ="""Corpus: {corpus}
Embedding dimension: {embed}
LSTM dimension: {lstm}
Hidden Dimension: {hidden}
Number of Epochs: {epoch}
Final loss:{loss}""".format(
                            corpus = input_corpus_filename,
                            embed= embed_dim,
                            lstm= lstm_dim,
                            hidden= hidden_dim,
                            epoch= epochs,
                            loss= epoch_loss)
print (outtable)
print("Accuracy: {}".format(correct/len(labeled_sentences_test)))


# In[ ]:


#Reminder for the summary


# In[ ]:


# Saving the model
modelfilename= "Models/{corpus}.{embed}emb.{lstm}lstm.{hidden}hid.{epoch}ep.model".format(
                            corpus = input_corpus_filename,
                            embed= embed_dim,
                            lstm= lstm_dim,
                            hidden= hidden_dim,
                            epoch= epochs)
torch.save(linguo.state_dict(), modelfilename)


# In[16]:


# Section reserver to save the data





# In[10]:


examplefile = open("example","w")


# In[11]:
