{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 sentences processed\n",
      "2000000 sentences processed\n",
      "3000000 sentences processed\n",
      "4000000 sentences processed\n",
      "5000000 sentences processed\n",
      "6000000 sentences processed\n",
      "7000000 sentences processed\n",
      "8000000 sentences processed\n",
      "9000000 sentences processed\n",
      "10000000 sentences processed\n",
      "11000000 sentences processed\n",
      "12000000 sentences processed\n",
      "13000000 sentences processed\n",
      "14000000 sentences processed\n",
      "15000000 sentences processed\n",
      "16000000 sentences processed\n",
      "17000000 sentences processed\n",
      "18000000 sentences processed\n",
      "19000000 sentences processed\n",
      "20000000 sentences processed\n",
      "The file has been fully processed, there are 5397947 sentences under 15 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Extract from the main corpus sentences no longer than cap\n",
    "# words\n",
    "cap = 15\n",
    "\n",
    "in_file_path = \"/Users/pablo/Downloads/escow16ax01.xml\"\n",
    "outfile_path = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/ESCOW/cap{cap}/ESCOWcap{cap}.sent_tag.all\".format(\n",
    "                                                                                                             cap=cap)\n",
    "outfile = open(outfile_path,\"w\")\n",
    "\n",
    "open_tag = re.compile(r\"<[^\\/].*>\")\n",
    "close_tag = re.compile(r\"<\\/.*>\")\n",
    "sentence = []\n",
    "tags = []\n",
    "accepted_sentences = 0\n",
    "total_sentences = 0\n",
    "for line in open(in_file_path,\"r\"):\n",
    "    if close_tag.match(line):\n",
    "        if len(sentence) <= cap and sentence[-1] in [\".\",\"?\",\"!\"]:\n",
    "            #write the sentence and the tags to the outfile\n",
    "            outline = \"{sent}|#|{tag}\\n\".format(sent = \" \".join(sentence),\n",
    "                                              tag = \" \".join(tags))\n",
    "            outfile.write(outline)\n",
    "            accepted_sentences += 1\n",
    "        sentence = []\n",
    "        tags = []\n",
    "        total_sentences += 1\n",
    "        if total_sentences % 1000000 == 0:\n",
    "            print(\"{} sentences processed\".format(total_sentences))\n",
    "    elif not open_tag.match(line):\n",
    "        elements = line.strip().split()\n",
    "        word = elements[0]\n",
    "        tag = elements[2]\n",
    "        sentence.append(word)\n",
    "        tags.append(tag)\n",
    "    \n",
    "\n",
    "\n",
    "summary = \"The file has been fully processed, there are {num} sentences under {cap} words\".format(\n",
    "                                                                                            num=accepted_sentences,\n",
    "                                                                                            cap=cap)\n",
    "print(summary)\n",
    "        \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6857713 under 15 without excluding by puntuation, from the 20894315 sentences in fold a of esCOW corpus\n",
    "5.397.947 under 15 excluding by puntuation, from the 20894315 sentences in fold a of esCOW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20894315\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def data_split(infile_path,outfile_pref,sizetrain=0.5, sizetest=0.30):\n",
    "    with open(infile_path,\"r\") as infile:\n",
    "        lines = infile.readlines()\n",
    "    numsent = len(lines)\n",
    "    cut1 = math.floor(sizetrain*numsent)\n",
    "    cut2 = math.floor(sizetest*numsent)\n",
    "    trainsents,testsents,valsents = lines[:cut1],lines[cut1:cut2],lines[cut2:]\n",
    "    trainfile_path = outfile_pref + \".train\"\n",
    "    with open(trainfile_path,\"w\") as trainfile:\n",
    "        trainfile.writelines(trainsents)\n",
    "    testfile_path = outfile_pref + \".test\"\n",
    "    with open(testfile_path,\"w\") as testfile:\n",
    "        testfile.writelines(testsents)\n",
    "    valfile_path = outfile_pref + \".val\"\n",
    "    with open(valfile_path,\"w\") as valfile:\n",
    "        valfile.writelines(valsents)\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file_path = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/ESCOW/cap20/ESCOWcap20.sent_tag.head1000\"\n",
    "outfile_pref = \"/Users/pablo/Dropbox/workspace/darth_linguo/Data/ESCOW/cap20/ESCOWcap20.sent_tag.head1000\"\n",
    "data_split(in_file_path,outfile_pref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import counter\n",
    "# Takes a training corpus and writes it's vocabulary to file, removing low frequency words\n",
    "# below a given threshold\n",
    "def getvocab(infile_path,lowfreq_thresh):\n",
    "    counts = counter\n",
    "    vocab = set()\n",
    "    for line in open(infile_path,\"r\"):\n",
    "        words_str , tags_str = line.split(\"|#|\")\n",
    "        words = words_str.split()\n",
    "        tags = tags_str.split()\n",
    "        for word in words:\n",
    "            counts[word.lower()] += 1\n",
    "        for tag in tags:\n",
    "            vocab.add(tag)\n",
    "    for word in counts:\n",
    "        if counts[word] <= lowfreq_thresh:\n",
    "            del counts[word]\n",
    "        else\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "def save_vocab(vocab,vocabfile_path):\n",
    "    with open(vocabfile_path)as vocab_file:\n",
    "        vocab_file.writelines(vocab)\n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
